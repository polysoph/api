{"categories":[{"id":0,"title":"Mathematics","slug":"mathematics"},{"id":1,"title":"Astronomy","slug":"astronomy"},{"id":2,"title":"Physics","slug":"physics"},{"id":3,"title":"Computer Science","slug":"computer-science"}],"grants":[{"id":6,"title":"Viability of Basic Income in the USA","description":"We’d like to fund a study on basic income—giving people enough money to live on with no strings attached. Although there’s been a lot of discussion, there’s little data about how it would work.\n\nMost nations have systems in place to give people resources, but bureaucratic and qualification requirements make it a very imperfect approximation of what most people mean when talking about a basic income. We have some examples of something close to a basic income in other countries, but we’d like to see how it would work in the US.\n"}],"keywords":[{"id":0,"title":"Combinatorics","slug":"combinatorics"},{"id":1,"title":"Prime Numbers","slug":"prime-numbers"},{"id":2,"title":"Erdogic Theory","slug":"erdogic-theory"},{"id":3,"title":"Functional Analysis","slug":"functional-analysis"}],"questions":{"dhj":{"id":0,"title":"Is there a simpler proof for the density Hales-Jewett theorem?","slug":"dhj-theorem","category":0,"config":{"tasks":false},"keywords":[0,1,2,3],"summary":"Can we develop a proof for the [DHJ](https://en.wikipedia.org/wiki/Hales%E2%80%93Jewett_theorem) using only basic mathematical principles?","description":"Discovered in 1975, the density Hales-Jewett theorem (DHJ) is a technique for estimating line-free configurations in multi-dimensional grids. Furstenberg and Katznelson 1991 paper laid out the first mathematical proof for the relationship. However, while their proof is sound, it relies on advanced techniques in the branch of mathematics called [ergodic theory](https://en.wikipedia.org/wiki/Ergodic_theory), which makes it difficult to follow.\n\n### Goal\n\nThis research project aims to use [combinatorial mathematics](https://en.wikipedia.org/wiki/Combinatorics) to discover a more simple proof for the density Hales-Jewett theorem.\n\n### Significance\n\nAn important part of mathematics is finding proofs using simpler techniques. Simpler proofs let us learn more about the relationships behind theorems. And in the case of the DHJ, it's close ties with other theorems mean that having a simpler proof could lead us to new understandings of prime numbers, which are used in everything from cryptography to dealing with large sets of data.\n\n### About the Theorem\n\nAlthough the proof of DHJ is complex, the basic statement can be understood by anyone. Take a look at the following three-by-three grid:\n\n![A 9x9 grid with circles in some of the spaces. A line goes through the right 3 spaces of the grid.](http://polysoph-assets.s3.amazonaws.com/dhj-lines-01.png)\n\nBy marking seven of the squares on the grid with a dot; as you can see, it’s possible to draw a line through three of those dots. By contrast, the configuration in the following picture is line-free—you can’t draw a line through any three of the dots:\n\n![](http://polysoph-assets.s3.amazonaws.com/dhj-lines-02.png)\n\nAfter a while, you discover that this configuration is the largest possible line-free configuration. In particular, if you mark seven dots on the grid, then no matter how you place the dots, it is always possible to draw a line through three of the dots, somewhere on the grid.\n\nThis logic extends into more dimensions. A three-dimensions 3x3 grid also contains a largest possible line-free configuration. And if we extend this into %n% dimensions, we eventually find that...\"\n","significance":"While the density Hales-Jewett theorem has been proven several times in the past (see [Szemerédi, 1975](#!) and [Furstenberg and Katznelson, 1991](#!)), the proofs have traditionally been long and drawn-out. Helping to find a simple proof can lead to a better understanding of the relationship between the DHJ-Szemerédi and Szemerédi–Green-Tao theorems, which relate closely to how we understand prime numbers. Why should you care about DHJ? DHJ connects to the problem of understanding the structure of the prime numbers. Finding new proofs can give us significant new insights that help us understand why a result is true in the first place. Indeed, this is exactly what happened with the multiple proofs of Szemerédi’s theorem.\n","metrics":{"followers":149,"citations":13},"citation":{"doi":"10.18258/3356"},"milestones":[{"id":0,"type":"begin","title":"Project Began","created_at":"2016-02-01T23:43:00.000Z","contents":"","owner_id":0,"comments":0,"views":500},{"id":1,"title":"Upper and Lower Bounds","created_at":"2016-02-09T15:12:00.000Z","contents":"Several of us have been working away on [the \"Lower Bounds\" discussion](/question/dhj-theorem/discussions/6), and have made some serious progress—but have now run into an equally serious issue. We started off exploring the relationship between the [Encyclopedia of Integer Sequences ](https://oeis.org/) values for 450 length sequences.\n\nFor any positive integer %n%, %let {}[3]^n% be the set of strings of length %n% consisting of %1%s, %2%s, and %3%s, and define a combinatorial line to be a triple of such strings arising by taking a string in %\\{1,2,3,x\\}^n% with at least one wildcard %x%, and substituting %x=1%, %x=2%, %x=3% in that string (e.g. %xx1x3% would give the combinatorial line %\\{11113, 22123, 33133\\}%).  Call a set %A \\subset [3]^n% of strings line-free if it contains no combinatorial lines, and let %c_n% be the size of the largest set %A% which is line-free. We then have arrive at:\n\n%%\\lim_{n \\to \\infty} c_n/3^n = 0%%\n\nThis theorem implies several other important results, most notably Roth’s theorem on length three progressions (in an arbitrary abelian group!) and also the corners theorem of Ajtai and Szemeredi (again in an arbitrary abelian group).\n\nHere are some of the questions we are initially exploring in the thread (which you can read through starting from comment [#3](http://localhost:8080/question/dhj-theorem/discussions/6#comment-1049) in the thread):\n\n1. What are the best upper and lower bounds for %c_n% for small %n%?  Currently we know %c_0=1%, %c_1=2%, %c_2=6%, %c_3=18%, %c_4=52%, with some partial results for higher %n%. We also have some relationships between different %c_n%.\n2. What do extremal or near-extremal sets (i.e. line-free sets with cardinality close to c_n) look like?\n3. What are some good constructions of line-free sets that lead to asymptotic lower bounds on c_n?  The best asymptotic lower bound we have currently is c_n \\geq 3^{n - O( \\sqrt{\\log n} )}.\n4. Can these methods extend to other related problems, such as the Moser cube problem or the capset problem?\n5. How feasible is it to extend the existing combinatorial proofs of Roth’s theorem (or the corners theorem), in particular the arguments of Szemeredi, to the Density Hales-Jewett theorem?\n6. But I imagine that further threads might develop in the course of the discussion.\n\nAs with the rest of the project, this is supposed to be an open collaboration: please feel free to pose a question or a comment, even if (or especially if) it is just barely non-trivial.\n","owner_id":1,"comments":0,"views":678},{"id":2,"title":"Quasirandomness","owner_id":0,"created_at":"2016-02-15T15:12:00.000Z","contents":"Many proofs of combinatorial density theorems rely in one way or another on an appropriate notion of quasirandomness. The idea is that, as we do, you have a dense subset of some structure and you want to prove that it must contain a substructure of a certain kind. You observe that a random subset of the given density will contain many substructures of that kind (with extremely high probability), and you then ask, “What properties do random sets have that cause them to contain all these substructures?” You look for some deterministic property of subsets of your structure that is sufficient to guarantee that a dense subset contains roughly the same number of substructures as a random subset of the same density.\n\nFor example, suppose you are trying to find a triangle in a dense graph. (We can think of a graph with %n% vertices as a subset of the complete graph with %n% vertices. So the complete graph %K_n% is our “structure” in this case.) It turns out that a quasirandomness property that gives you the right number of triangles is this: a graph of density %\\alpha% is quasirandom if the number of quadruples %(x_1,x_2,x_3,x_4)% such that %x_1x_2,x_2x_3,x_3x_4% and %x_4x_1% are all edges is approximately %\\alpha^4n^4%. That is, the number of labelled 4-cycles is roughly what it would be in a random graph. If this is the case, then it can be shown that it is automatically also the case that the number of triples %(x_1,x_2,x_3)% such that %x_1x_2,x_2x_3% and %x_3x_1% are edges is approximately %\\alpha^3n^3%, again the number that you would expect in a random graph.\n\nThis might seem a slightly disappointing answer to the question. We are trying to guarantee the expected number of one small subgraph and we assume as our quasirandomness property something that appears to be equally strong: that we have the expected number of another small subgraph. How can that possibly be a good thing to do?\n\nA first answer is that if you have the right number of 4-cycles then it doesn’t just give you triangles: it gives you all small subgraphs. But an answer that is more pertinent to us here is that we can draw very useful consequences if a graph does not have the right number of 4-cycles. For example, we can find two large sets of vertices %A% and %B% such that the number of pairs %(x,y)\\in A\\times B% such that %xy% is an edge is significantly larger than %\\alpha|A||B|% (which is what we would get for a random graph). Note that this is a global conclusion (an over-dense bipartite subgraph) deduced from a local hypothesis (the wrong number of 4-cycles). We can turn this round and say that we have an alternative definition of quasirandomness: a graph of density %\\alpha% is quasirandom if for any two large sets %A% and %B% of vertices, the number of pairs %(x,y)\\in A\\times B% such that %xy% is an edge is about %\\alpha |A||B|%. This definition turns out to be equivalent to the previous definition.\n\nThis sort of local-to-global implication is extremely useful. The local property can be used to show that sets have substructures. And if the local property fails, then one has some global property of the set that can be exploited in a number of ways.\n\nIt is not too important for the purposes of this thread to understand what all these ways are. But one of them does stand out: a global property is useful if it allows you to find a substructure of the main structure such that the intersection of %A% with that substructure is denser than it would be for a random set. In the graphs case, our substructure will be a large complete bipartite graph. The obvious substructure to hope for in the density Hales-Jewett set-up is a combinatorial subspace (which is like a combinatorial line except that you have several different variable sets, with the number of these sets being called the dimension of the combinatorial subspace).\n","comments":3,"views":326},{"id":5,"title":"Possible Proof Strategies","created_at":"2016-02-28T17:42:00.000Z","owner_id":0,"contents":"I'll try to keep this summary brief, instead linking to comments that expand on what I say. If we take our lead from known proofs of Roth’s theorem and the corners theorem, then we can discern several possible approaches (each one attempting to imitate one of these known proofs).\n\n**Szemerédi’s Original Proof** ([Discussion](#!))\n\nSzemerédi proved a proof of a one-dimensional theorem. I do not know whether his proof has been modified to deal with the multidimensional case (this was not done by Szemerédi, but might perhaps be known to Terry, who recently digested and rewrote Szemerédi’s proof). So it is not clear whether one can base an entire argument for DHJ on this argument, but it could well contain ideas that would be useful for DHJ. For a sketch of Szemerédi’s proof, see [this post](http://example.com) of @ttao’s. He also has links to comments that expand on what he says.\n\nAjtai and Szemerédi found a clever proof of the corners theorem that used Szemerédi’s theorem as a lemma. This gives us a direction to explore that we have hardly touched on. Very roughly, to prove the corners theorem you first use an averaging argument to find a dense diagonal. For any two such points %(x,r-x)% and %(y,r-y)% you know that the point %(x,r-y)% does not lie in %A% (assuming %A% is corner-free), which gives you some kind of non-quasirandomness.\n\nIt allows you to find a large Cartesian product %X \\times Y% in which %A% is a bit too dense. To exploit this, Ajtai and Szemerédi used Szemerédi’s theorem to find long arithmetic progressions %P \\subset X% and %Q \\subset Y% of the same common difference so that %A% has a density increment in %P \\times Q%. So we could think about what the analogue would be in the DHJ setting. Presumably it would be some kind of multidimensional Sperner statement of sufficient depth to imply Szemerédi’s theorem. A naive suggestion would be that in a dense subset of %{}[2]^n% you can find a large-dimensional combinatorial subspace in which all the variable sets have the same size. If you apply this to a union of layers, then you find an arithmetic progression of layer-cardinalities. But this feels rather artificial, so here’s a question we could think about.\n\n**Density-increment Strategies** ([Discussion](#!))\n\nThe idea here is to prove the result by means of the following two steps.\n\n1. If %A% does not contain a combinatorial line, then it correlates with a set %S% with some kind of atypical structure that we can describe.\n2. Every set %S% with that kind of structure can be partitioned into (or almost evenly covered by) a collection of combinatorial subspaces with dimension tending to infinity as %n% moves to infinity.\n\nThen, if %A% contains no combinatorial line, then find %S% such that %A% is a bit denser in %S% than it is in %{}[3]^n%. Cover %S% with subspaces. By averaging we find that the density of %A% is a bit too large in one of these subspaces. But now we are back where we started with a denser set %A% so we can repeat. The iteration must eventually terminate (since we cannot have a density greater than 1) so if the initial %n% was large enough then we must have had a combinatorial line.\n\n**Triangle Removal** ([Discussion](/question/dhj-theorem/discussions/0))\n\nThis was the initial proposal, and we have not been concentrating on it recently, so I will simply defer to [the original post](/question/dhj-theorem/discussions/0), and add the remark that if we manage to obtain a complete and global description of [obstructions to uniformity](/question/dhj-theorem/discussions/4), then regularity and triangle removal could be an alternative way of using this information to prove the theorem. And in the case of corners, this is a somewhat simpler thing to do.\n\n**Ergodic-inspired methods** ([Discussion](#!))\n\nFor a proposal to base a proof on at least some of the ideas that come out of the proof of Furstenberg and Katznelson, see @ttao's comment 439, as well as the first few comments after it. @ttao has also begun an [online reading seminar](http://terrytao.wordpress.com/2009/02/11/a-reading-seminar-on-density-hales-jewett/) on the Furstenberg-Katznelson proof, using not just the original paper of Furstenberg and Katznelson but also a more recent paper of Randall McCutcheon that explains how to finish off the Furstenberg-Katznelson argument.\n\nP.S. We've heard that this platform is going to be adding a wiki feature soon. We'll keep you posted, and we'll be able to start documenting things there.\n","comments":5,"views":435}],"owners":[{"user_id":0},{"user_id":1}],"discussions":[{"id":0,"title":"Initial Work","slug":"initial-work","status":"resolved","created_at":null,"created_by":0,"timeline":[{"id":1,"type":"comment","ref":401},{"id":2,"type":"comment","ref":402},{"id":3,"type":"comment","ref":403}],"comments":[{"id":401,"owner_id":0,"created_at":"2016-02-01T13:43:00.000Z","contents":"To get the ball rolling, I'll be giving my initial thoughts in small chunks.\n\nMight it be possible to prove the density Hales-Jewett theorem for %k=3% by imitating the triangle-removal proof that dense subsets of %[n]^2% contain corners? If one were going to do that, what would the natural analogue of the tripartite graph associated with a dense subset be? If we’ve got a dense subset %A% of %[3]^n%, then how might we define a tripartite graph %G% in such a way that triangles in %G% correspond to (possibly degenerate) combinatorial lines?\n\nIt’s perhaps worth pointing out that there is a natural definition of a degenerate combinatorial line: it’s one where the set of coordinates that vary from 1 to 3 is empty. If we just look at the vertex sets %X% and %Y% in the case of subsets %A\\subset [n]^2%, we have a very simple definition for an edge: it’s a pair %(x,y)% that belongs to %A%.\n"},{"id":402,"owner_id":0,"created_at":"2016-02-01T13:45:00.000Z","contents":"Does %[3]^n% split up nicely as a Cartesian product? You can write it as %[3]^r\\times [3]^s%, but that doesn’t help much, because if we represent a typical point as %(x,y)% with %x\\in [3]^r% and %y\\in [3]^s%, then a Hales-Jewett line is not a triple of the form %(x,y)%, %(x+d,y)%, %(x,y+d)%. That’s partly because we can’t even make sense of adding %d%, but more fundamentally because even if we could invent a useful definition of addition in this context, we still wouldn’t get a Hales-Jewett line."},{"id":403,"owner_id":0,"created_at":"2016-02-01T13:49:00.000Z","contents":"By the way, there’s a nice way of deducing the corners result from density Hales-Jewett: Suppose we take a set %A\\subset [3]^n% with the special property that whether or not a sequence %x% belongs to %A% depends only on the numbers of %1%s, %2%s and %3%s in %x%. That is, let %T% be the set of all triples %(r,s,t)% of non-negative integers such that %r+s+t=n%, let %S% be some subset of %T%, and let %A% be the set of sequences %x% such that %(n_1(x),n_2(x),n_3(x))\\in S%, where %n_i(x)% stands for the number of is in %x%. Now suppose you have a combinatorial line %L%. If %d% is the number of variable coordinates in %L%, then %S% must contain a triple of points of the form %(r+d,s,t)%, %(r,s+d,t)%, %(r,s,t+d)% (where %r%, %s% and %t% are the numbers of %1%s, %2%s and %3%s amongst the fixed coordinates of %L%).\n\nNow %T% is a large triangular subset of a triangular grid, and one can (if one really feels like it) shear it so that it becomes a right-angled triangle instead of an isosceles triangle. This way it's not hard to show that the assertion that every dense subset of %T% contains the vertices of an equilateral triangle of the form %(r+d,s,t)%, %(r,s+d,t)%, %(r,s,t+d)% is equivalent to the corners result.\n\nUnfortunately it’s not the case that dense subsets of %T% correspond to dense subsets of %[3]^n% (because almost every point in %[3]^n% has similar numbers of %1%s, %2%s and %3%s) but there are dodges for getting round that.\n"}]},{"id":1,"title":"Triangle Removal","slug":"triangle-removal","status":"open","created_at":"2016-02-02T02:08:00.000Z","created_by":2,"timeline":[{"id":234,"type":"comment","ref":2},{"id":454,"type":"comment","ref":9},{"id":463,"type":"comment","ref":74},{"id":335,"type":"comment","ref":75},{"id":452,"type":"comment","ref":85},{"id":353,"type":"comment","ref":95},{"id":356,"type":"comment","ref":96}],"comments":[{"id":2,"owner_id":2,"created_at":"2016-02-02T02:08:00.000Z","contents":"We should consider a variant of the original problem first. If the removal technique doesn’t work here, then it won’t work in the more difficult setting. If it works, then we have a nice result! Consider the Cartesian product of an %IP_d% set.\n\n> An %IP_d% set is generated by %d% numbers by taking all the %2^d% possible sums. So, if the %n% numbers are independent then the size of the %IP_d% set is %2^d%. In the following statements we will suppose that our %IP_d% sets have size %2^n%.\n\nProve that for any %c>0% there is a %d%, such that any %c%-dense subset of the Cartesian product of an %IP_d% set (it is a two dimensional pointset) has a corner.\n\nThe statement is true. One can even prove that the dense subset of a Cartesian product contains a square, by using the density HJ for %k=4%\\. (I will sketch the simple proof later) What is promising here is that one can build a not-very-large tripartite graph where we can try to prove a removal lemma. The vertex sets are the vertical, horizontal, and slope -1 lines, having intersection with the Cartesian product. Two vertices are connected by an edge if the corresponding lines meet in a point of our c-dense subset. Every point defines a triangle, and if you can find another, non-degenerate, triangle then we are done. This graph is still sparse, but maybe it is well-structured for a removal lemma.\n\nFinally, let me prove that there is square if %d% is large enough compare to %c%. Every point of the Cartesian product has two coordinates, a %0,1% sequence of length %d%. It has a one to one mapping to %[4]^d%; Given a point %((x_1,\\ldots,x_d),(y_1,\\ldots,y_d))% where %x_i,y_j% are 0 or 1, it maps to %(z_1,…,z_d)%, where %z_i=0% if %x_i=y_i=0%, %z_i=1% if %x_i=1% and %y_i=0%, %z_i=2% if %x_i=0% and %y_i=1%, and finally %z_i=3% if %x_i=y_i=1%. Any combinatorial line in %[4]^d% defines a square in the Cartesian product, so the density HJ implies the statement."},{"id":9,"owner_id":0,"created_at":"2016-02-02T06:36:00.000Z","contents":"@ttao's comment about Le Thai Hoang’s observation (#4) seems to me to be an important one, so let me spell out why, even though it’s basically there in what Terry says. If you follow the triangle-removal proof of the corners result, you find that you can prove something stronger: in a set of density %\\delta% you don’t just get one corner, but you get %c(\\delta)n^2% corners. (The proof is that if there are very few corners then there are very few triangles in the associated tripartite graph, and then you can apply triangle removal just as before to get a contradiction.) If one is going to find an analogous proof here, it is essential that it should not try to prove a lemma that has false consequences. In our case, we must be careful that we don’t accidentally try to develop an approach that would also imply that a positive density of all possible combinatorial lines lie in the set.\n\nI haven’t checked, but I think that weighting the vertices as suggested in KK avoids this pitfall, because now the analogous set to the one used by Le Thai Hoang is concentrated around sets of size %n/3%. Probably this is exactly what Terry meant at the end of his second paragraph."},{"id":74,"owner_id":0,"created_at":"2016-02-03T20:26:00.000Z","contents":"Actually, I was trying to prove a Varnavides-type result not because we need it but because it was a sort of reality check: if we can’t get such a result then it is unlikely that triangle removal will work. But since I’m now fairly confident that we can get such a result, I want to see what happens if one just goes ahead and tries to apply some kind of triangle-removal lemma in the case where %A% is a dense subset of a large triangular grid of slices. So let’s fix notation again. I’m going to suppose that %A% is a dense subset of the union of all slices %\\gamma_{a+r,b+s,c+t}%, where %(a,b,c)% is some fixed element of %T_{n-m}% and %(r,s,t)% ranges over %T_m%. Also, I’m assuming that %a,b,c% are all around %n/3% so that all slices have about the same size. Now I want to apply the trick from comments N and P. So what is the graph now? Instead of taking %X% to be the full power set of %[n]% I want to take just those sets that could conceivably occur as the set of 1s in a sequence in %A%. And that’s all sets of size %a+r%, where %0\\leq r\\leq m%. Similarly, %Y% consists of all sets of size between %b% and %b+m%, and %Z% consists of all sets of size between %c% and %c+m%. The edges are as before: I join %U\\in X% to %V\\in Y% if the sequence that’s 1 on %U% and 2 on %V% and 3 everywhere else belongs to %A%. And similarly for the other two bipartite graphs. And clearly a non-degenerate triangle in this graph gives rise to a combinatorial line. I’ll think about the implications of this in my next comment."},{"id":75,"owner_id":0,"created_at":"2016-02-03T20:44:00.000Z","contents":"I didn’t mention it explicitly, but of course a necessary condition for joining %U\\in X% to %V\\in Y% is still that %U% and %V% should be disjoint. So the first reason that we are not yet done is that the graphs are still very sparse, since two random sets of size %n/3% have a tiny probability of being disjoint. So the question is, has anything been gained by restricting attention to a large triangular grid of slices? Something’s been gained from the Varnavides point of view, I think, but has anything been gained from the triangle-removal point of view?\n\nAt the moment it feels, rather disappointingly, as though _nothing_ has been gained. It’s still the case that the graph where you join two sets if they are disjoint is importantly non-quasirandom, since the remark made in comment LL carries over to this context too. I would hazard a guess that there are two morals to draw from this.\n\n1. It may still be quite convenient to restrict to a triangular grid of slices, in order to make certain distributions uniform.\n2. We are probably forced to consider obstructions to uniformity. In fact, there’s a different way of justifying.\n3. At some point, if a triangle-removal argument is going to work, we will have to prove a statement to the effect that if you have three bipartite graphs that are all “regular” and “dense”, then they must give rise to lots of triangles.\n\nIn the normal dense-graphs case, “regular” just means quasirandom, and it even suffices for just one of the three bipartite graphs to be regular, provided there are plenty of vertices in the third set with high degree in both of the first two sets. In our case, there is a one-to-one correspondence between edges of one of the bipartite graphs and points in %[3]^n%. So this suggests that at some point we are going to have to understand what a “regular” or “quasirandom” subset of %[3]^n% is like. To be continued in the next comment."},{"id":85,"owner_id":1,"created_at":"2016-02-03T23:46:00.000Z","contents":"@Sune #59: The lower bound %c_4 \\geq 49% you have compares pretty well with the upper bound %c_4 \\leq 3c_3 = 54%. It may be possible to shave the upper bound down a notch, by trying to figure out what the extremal subsets of %{}[3]^3% are that have 18 elements but no combinatorial lines. Any 54-element subset of %{}[3]^4% with no combinatorial lines must be extremal on every three-dimensional slice, which looks unlikely if there are few extremals."},{"id":95,"owner_id":0,"created_at":"2016-02-04T04:18:00.000Z","contents":"That's an important question, but I’d like to understand more precisely what you mean by it.\n\nTo begin with, let me distinguish carefully between two interpretations of the question. One would be “Is it possible to do without any analogue of the triangle removal lemma from this point on?” and another would be “Is it unnatural to try to use the triangle removal lemma from this point on?”\n\nThe background to the question is that if a positive proportion of the slices is rich, then by the corners theorem one can find a triple of rich slices of the form %\\gamma_{a+r,b,c}%, %\\gamma_{a,b+r,c}%, %\\gamma_{a,b,c+r}%. So one might take the view that you use the triangle removal lemma (via the corners theorem) to get yourself to a triple of rich slices and then let other methods take over from there. Now we haven’t actually said what we mean by “rich”. It has to mean more than “intersects densely with %A%” since it is easy to find examples where %A% intersects three such slices densely but there is no combinatorial line. (See e.g. comment 20.) And it seems unlikely that there is a definition of “rich” such that:\n\n1. Every dense set %A% intersects a dense set of slices richly and\n2. Given any triangle of slices intersected richly by %A% you must get a combinatorial line from that triangle of slices.\n\n If you don’t get a dense set of rich slices, then perhaps you can pass to substructures (e.g. by fixing a few coordinates) where something like density or energy improves and try again. This would be a sort of mixed argument: first use density/energy-increase arguments to get to a situation where lots of slices are rich (which I see as some kind of quasirandomness property, but perhaps I shouldn’t), then apply the corners theorem to get a triple of rich slices, and finally apply some rich-slices counting lemma to get a combinatorial line. (I’m not at all sure that that’s what Terry had in mind, and would be interested to know.)\n\nAnyhow, perhaps that suggests that triangle removal could be used just to get a triple of good slices, and other arguments could be used to do the rest. In my next comment I’ll discuss the other interpretation of Terry’s question."},{"id":96,"owner_id":0,"created_at":"2016-02-04T04:53:00.000Z","contents":"Is it clearly wrong to try to prove the whole theorem via a triangle-removal lemma? I think that the argument gains considerably in force because it is possible to define a tripartite graph in such a way that nondegenerate triangles correspond to combinatorial lines in %A%. Moreover, the construction naturally generalizes the tripartite graph that you construct in order to deduce the corners theorem from the triangle removal lemma.\n\nHere’s a way one might justify this. For a long time it was known that the triangle removal lemma implied Roth’s theorem, but until Jozsef came along, nobody had observed that it would also do corners. Before that one might have reasoned as follows. Let us label the lines of slope 1 in %[n]^2% according to the value of %x-y%. If we have a corner, then the labels of the lines that it lies in must form an arithmetic progression of length 3.\n\nAnd indeed, since our dense subset of %[n]^2% might even _be_ a union of lines of slope 1, we see that the corners theorem implies Roth’s theorem. So perhaps we should use Roth’s theorem to find a rich triple of diagonals and then use other arguments to obtain a corner. Is this a fair analogy or does it break down somewhere? Before giving up on triangle removal, I would want answers to the following questions:\n\n1. Is it possible to formulate a triangle-removal statement that isn’t obviously false and that implies that a tripartite graph contains lots of triangles (because it contains lots of edge-disjoint degenerate ones)?\n2. If so, is there the slightest chance of proving the statement by imitating the proof of the usual triangle-removal lemma? Or does the sparseness and nonquasirandomness of the bipartite graph where we join two sets if they are disjoint make a project of this kind hopeless? I could perhaps add a third question, of a slightly different kind.\n3. Is it the case that in order to prove that a regular triple contained many triangles, one would have to develop tools (connected with obstructions to uniformity) that could be used to give an easier proof that didn’t go via triangle removal, regularity lemmas etc.?\n\nI feel reasonably optimistic about (1) if one restricts to a triangular grid of slices. I’m much less sure about (2) and (3)."}]},{"id":2,"title":"Varnavides Approach","slug":"varnavides-approach","created_at":"2016-02-02T09:08:00.000Z","created_by":1,"status":"resolved","timeline":[{"id":256,"type":"comment","ref":13},{"id":257,"type":"comment","ref":14},{"id":258,"type":"comment","ref":16},{"id":259,"type":"comment","ref":17},{"id":260,"type":"comment","ref":18},{"id":261,"type":"comment","ref":24},{"id":262,"type":"comment","ref":25},{"id":263,"type":"comment","ref":63},{"id":264,"type":"comment","ref":64}],"comments":[{"id":13,"owner_id":1,"created_at":"2016-02-02T09:08:00.000Z","contents":"Perhaps in order to understand the issue coming from Hoang’s observation a bit better, one can pose the following subproblem: what is the natural analogue of Varnavides theorem for DHJ? Varnavides used Roth’s theorem as a black box to show that any subset of %{}[n]% of density %\\delta% contained %\\geq c(\\delta) n^2% 3-term arithmetic progressions when n was large; a similar argument using the Ajtai-Szemeredi theorem as a black box also shows that any subset of %{}[n]^2% of density %\\delta% contains %\\geq c(\\delta) n^3% corners.\n\nSo, using DHJ as a black box, what is the natural implication as to how “numerous” combinatorial lines are in a dense set? It seems that direct cardinality of such lines may not be the best measure of being “numerous”, but there should presumably still be some Varnavides-like theorem out there."},{"id":14,"owner_id":2,"created_at":"2016-02-02T10:26:00.000Z","contents":"Regarding @ttao's last comment about a Varnavides-like theorem, I’m a bit skeptical. Even for the %k=2% case (Sperner’s thm) if we take points of the d dimensional cube where the difference between the 0-s and 1-s is at most %\\sqrt{d}% then we have a positive fraction of the elements and there are less than %\\binom{d/2}{\\sqrt{d}}2^d% combinatorial lines. Probably this should be the right magnitude for %k=2% as a Varvadines-like result and it shouldn’t be difficult to prove. (But I don’t have the proof …)"},{"id":16,"owner_id":1,"created_at":"2016-02-02T13:00:00.000Z","contents":"Hmm, you’re right; so we get a bound of %c_n \\gg 3^n / \\sqrt{n}% asymptotically.\n\n@jozsef, yes, we will have to normalise things appropriately to get a Varnavides type theorem. For the %k=3% problem, for instance, one might try to shoot for a lower bound for the number of combinatorial lines which is something like %(3+o(1))^n% rather than %4^n%. Alternatively, we could somehow weight each line, perhaps by some factor depending on how many wildcards it contains.\n\n(Note that any set of density %\\delta% in a large %{}[3]^n% will contain at least one combinatorial line in which the number of wildcards is at most %C(\\delta)%, since one can partition %{}[3]^n% into copies of %{}[3]^{n_0}%, where %n_0% is the first index for which DHJ holds at density %\\delta/2%, and observe that the original set will have density at least %\\delta/2% in at least %\\delta/2% of these copies. So maybe one has to weight things to heavily favour lines with very few wildcards.)"},{"id":17,"owner_id":1,"created_at":"2016-02-02T13:46:00.000Z","contents":"It just occurred to me that if a subset of %{}[3]^n% has positive density, then by the central limit theorem we may restrict attention to those strings which have %n/3+O(\\sqrt{n})% 1’s, 2’s, and 3’s, and still have a positive density of strings remaining. So without loss of generality we may assume that all strings in the set are “balanced” in this manner. Hence, the only combinatorial lines which really matter are those which have %O(\\sqrt{n})% wildcards in them, rather than %O(n)%. So the number of combinatorial lines that are genuinely “in play” is just %(3+o(1))^n% rather than %4^n%. I wonder if DHJ implies the existence of a combinatorial line with %\\Theta(\\sqrt{n})% wildcards. In the k=2 case it seems that a double counting argument should be able to establish something like this, though I haven’t checked it thoroughly. If so, then there seems to be a good shot at getting the “right” Varnavides type theorem."},{"id":18,"owner_id":1,"created_at":"2016-02-02T14:01:00.000Z","contents":"Two more thoughts… firstly, a sufficiently good Varnavides type theorem for DHJ may have a separate application from the one in this project, namely to obtain a “relative” DHJ for dense subsets of a sufficiently pseudorandom subset of %{}[3]^n%, much as I did with Ben Green for the primes (and which now has a significantly simpler proof by @gowers and by Reingold-Trevisan-Tulsiani-Vadhan). There are other obstacles though to that task (e.g. understanding the analogue of “dual functions” for Hales-Jewett), and so this is probably a bit off-topic.\n\nAnother thought: For any %a,b,c% adding up to %n%, let %\\gamma_{a,b,c}% be the subset of %{}[3]^n% consisting of those strings with a 1s, b 2s, and c 3s. If A is a dense subset of %{}[3]^n%, then %A \\cap \\gamma_{a,b,c}% should be a dense subset of %\\gamma_{a,b,c}% for many a, b, c; call a triple (a,b,c) _rich_ if this is the case. By applying the corners theorem (!) we should then be able to find %(a+r,b,c)%, %(a,b+r,c)%, %(a,b,c+r)% which are simultaneously rich. But now this might be a good place with which to try a triangle removal argument, converting the lines of r wildcards connecting %\\gamma_{a+r,b,c}, \\gamma_{a,b+r,c}, \\gamma_{a,b,c+r}% into triangles as Tim suggests in the main post?"},{"id":24,"owner_id":0,"created_at":"2016-02-02T17:45:00.000Z","contents":"I want to continue the discussion in #13, #14, #16, #17 and #18 concerning the appropriate Varnavides-type theorem for density Hales-Jewett. The basic difficulty appears to be that if you know that a point lies in a combinatorial line, then it affects the shape of the point.\n\nLet’s see this first in a very simple context, where we just put the uniform distribution on the set of all points and the uniform distribution on the set of all lines. We can represent a combinatorial line as a point in the space %\\{1,2,3,*\\}^n%, where the asterisks represent the varying coordinates. In this way we see easily that there are %4^n% combinatorial lines and that the set of fixed coordinates that take a given one of the values 1, 2 or 3 will typically have size about %n/4%. (For anyone who isn’t familiar with this kind of argument, if you choose a random combinatorial line, regarded as a sequence that consists of 1s, 2s, 3s and asterisks, then the set of places where you choose 1 is a random subset of %[n]% where each point is chosen with probability %1/4%. With very high probability such a set has size close to %n/4%.) So we see that while a typical point has roughly equal numbers of 1s, 2s and 3s, a typical combinatorial line contains points that have around %n/4% of two values and around %n/2% of the third value.\n\nSuppose we try to remedy this by putting a different distribution on to the set of lines. Is there some natural way to do this that encourages the set of variable coordinates to be much smaller than the sets of fixed coordinates? A rather unpleasant approach would be simply to put some upper bound on the size of the set of variable coordinates and otherwise choose uniformly. The most natural thing I can think of to do is not all that much nicer, but at least it smooths the cutoff. It’s to choose the variable set of coordinates by picking each point independently with probability %p%, where %p% is something like %n^{-1/2}%, and then choose the fixed coordinates randomly to be 1, 2 or 3\\. Actually, I can already see that this doesn’t work. The size of the resulting set of variable coordinates will be strongly concentrated around %n^{1/2}%. But it’s not hard to choose a dense set of sequences such that if you choose any two of them, %x% and %y%, then the number of 1s in %x% and the number of 1s in %y% do not differ by approximately %n^{1/2}%. The number of combinatorial lines in such a set would not be dense. The fact that the natural measure on the set of sequences does not project properly to the natural measure on the set of triples %(r,s,t)% of integers such that %r+s+t=n% seems to be a fundamental difficulty. Another thought one might have is to put a different measure on the set of points, so as to encourage points where we get two values roughly %n/4% times and one value roughly %n/2% times. But the trouble with this is that it leads to the no-degenerate-triangles problem. To be precise, it would lead to three sets of sequences (according to whether 1, 2 or 3 was the preferred value) and no sequence would belong to all three sets. So it really does seem essential to force the variable set to be small somehow. And all I have to suggest so far is to do something very unnatural like first randomly choosing the cardinality according to some distribution that isn’t too concentrated about any value — the most natural one I can think of is a gentle exponential decay — then choosing a set of that cardinality, and then choosing the fixed coordinates randomly. But I have no reason to suppose that that leads to a Varnavides-type theorem."},{"id":25,"owner_id":0,"created_at":"2016-02-02T18:44:00.000Z","contents":"I see now that there’s quite a substantial overlap between my last comment and #16\\. But one can view my last comment as an elaboration of his, perhaps.\n\nHere I just want to make the quick suggestion that one way to work out what the appropriate Varnavides theorem should say is to prove it first. How would this strategy work? Well, we assume the density Hales-Jewett theorem in the following form: for every %\\delta>0% there exists %k% such that every subset of %[3]^k% of size at least %(\\delta/2)3^k% contains a combinatorial line. (The %\\delta/2% is convenient in just a moment.)\n\nNow let %A% be a subset of %[3]^n% of size at least %\\delta 3^n%. Here, %n% is an integer that is much bigger than %k%. The set %[3]^n% can be covered uniformly by structures that are isomorphic to %[3]^k%, and a positive proportion (in fact, %\\delta/2% at least) of these structures intersect %A% in a subset of density at least %\\delta/2%. So a positive proportion of these structures contain a combinatorial line. And now a double-counting argument gives us lots of combinatorial lines.\n\nNow there are several choices to make if one wants to pursue this line of attack. The main one is what set of %k%-dimensional substructures of %[3]^n% to average over. For the benefit of people who haven’t thought too hard about Hales-Jewett, let me give a pretty general class of such structures. You choose %k% disjoint sets %V_1,V_2,\\ldots,V_k%, and you choose fixed values in %\\{1,2,3\\}% for all elements of %[n]% that do not belong to one of the %V_i%. The %k%-dimensional structure consists of the %3^k% sequences that take the fixed values outside the %V_i% and are constant on each %V_i%. What can we deduce from the fact that there exists %k% that depends on %\\delta% only such that if %A% intersects any %k%-dimensional set of this kind in at least %(\\delta/2)3^k% places then that %k%-dimensional set contains a combinatorial line in %A%?\n\nThe choice one has is this: over what set of structures of this kind (or with what weighting on the structures) do we do an averaging argument? And what can we expect to get out of it at the end? Is it likely to give anything useful?\n\nOne other question: is there a reasonably precise argument to the effect that if the triangle-removal approach worked, then it would imply a stronger Varnavides-type result? And if so, what would that result be? This could be another way to get a handle on the problem."},{"id":63,"owner_id":1,"created_at":"2016-02-03T08:04:00.000Z","contents":"I thought it might also be worth reporting here on some things I tried that _don’t_ seem to work, though perhaps I am missing something. Firstly, I tried improving the lower bound we now have on %c_n% by not taking the global statistics a, b, c of 1s, 2s, and 3s (and their resulting sets %\\gamma_{a,b,c}%, but instead partitioning [n] into subsets, using the local statistics for each subsets to create local versions of the %\\gamma%‘s, and then somehow gluing it all back together again. This failed to improve upon what we already have, and I think the reason is while the global %\\gamma_{a,b,c}% have no combinatorial lines, the same is not true of their local analogues (unless one takes a Cartesian product of several local %\\gamma%‘s, but this seems to be rather lossy.) Indeed, I still don’t know of any good example of a really large set with no combinatorial lines that isn’t somehow built out of the %\\gamma%‘s. The other thing I toyed with was trying to use the machinery of shifting and compression (in the spirit of Frankl, or Bollobas-Leader, or one of my papers with Ben Green) to try to get good answers to the k=2 problems (e.g. finding a good “robust” version of Sperner’s theorem, e.g. a Varnavides version). But this seems not to work too well. The problem seems to be that the extremisers for Sperner’s theorem don’t look much like downsets (they seem to be more like the boundary of a downset, though I don’t see how to exploit this). In any event, these techniques are quite specific to k=2 and would be unlikely to shed much light on k=3 except perhaps by analogy."},{"id":64,"owner_id":6,"created_at":"2016-02-03T08:28:00.000Z","contents":"On the topic of more analytic proofs of Sperner: I think I now see why Boris mentioned Kruskal-Katona in the same breath as Sperner. Let me try to sketch the connection I see. I’m sure it’s well known. Let %n% be odd for simplicity and write %B_j% for the set of strings in %\\{0,1\\}^n% with exactly %j% %1%‘s. Let %k = \\lfloor n/2 \\rfloor%. Let %A_j% denote %A \\cap B_j% and let %\\mu_j% denote %A_j%‘s density within %B_j%. One particular thing Sperner tells us is that if %\\mu_k + \\mu_{k+1} > 1% then %A% contains a “combinatorial line” (because the overall density of %A% is at least that of %B_{k}%‘s density within %\\{0,1\\}^n%). I.e., there is %x \\in A_k, y \\in A_{k+1}% such that %x \\prec y%. Here is a way to see that with Kruskal-Katona: K-K implies that the upper-shadow %\\partial^+ A_k% of %A_k% has density at least %\\mu_k% within %B_{k+1}%. Now together %\\partial^+ A_k% and %A_{k+1}% have density exceeding %1% within %B_{k+1}%, hence they overlap. — Indeed, I think you can get most of Sperner out of extending this argument. Let me be uncareful. Suppose that %A%‘s overall density exceeds that of %B_{k+1}% within %\\{0,1\\}^n%. As an example, perhaps %A% has %40\\%% of the strings in %B_{k}%, %40\\%% of the strings in %B_{k+5}% and %40\\%% of the strings in %B_{k+7}%. Now K-K implies that %A_{k}%‘s upper shadow is also at least %40\\%% of %B_{k+1}%. And the upper shaddow of that is at least %40\\%% (basically) of %B_{k+2}%. Etc. When you get up to %B_{k+5}%, you either get a collision and are done, or you have at least %80\\%% of %B_{k+5}%. Then you get at least %80\\%% of %B_{k+6}%, and then you have %120\\%% of %B_{k+7}% and you get a collision and are done. — The reason I kind of like this proof is that there is an “analytic” proof of Kruskal-Katona that seems amenable to “robustification”. Although to be honest, I can’t quite remember how we got talking about this subject just now…"}]},{"id":3,"title":"A Hyper Optimistic Conjecture","slug":"a-hyper-optimistic-conjecture","created_at":"0001-02-20T05:00:00.000Z","created_by":1,"status":"open","timeline":[{"id":3411,"type":"comment","ref":237},{"id":3413,"type":"comment","ref":238},{"id":3414,"type":"comment","ref":239},{"id":3415,"type":"comment","ref":240},{"id":3416,"type":"comment","ref":241},{"id":3417,"type":"comment","ref":242},{"id":3418,"type":"comment","ref":244},{"id":3418,"type":"comment","ref":245},{"id":3419,"type":"action","ref":7},{"id":3419,"type":"comment","ref":246},{"id":3421,"type":"comment","ref":247},{"id":3420,"type":"action","ref":8}],"comments":[{"id":237,"owner_id":1,"created_at":"2016-02-20T05:00:00.000Z","contents":"Over in the [other thread](/question/dhj-theorem/discussions/0#comment-9), @gil and @gowers have proposed a “hyper-optimistic” conjecture, and asked whether it could be falsified using the examples from this thread. Let me rephrase so it's easier to follow.\n\nGiven a set %A \\subset {}[3]^n%, define the _weighted size_ %\\mu(A)% of A by the formula:\n%%\\mu(A) := \\sum_{a+b+c=n} |A \\cap \\Gamma_{a,b,c}|/|\\Gamma_{a,b,c}|%%\nThus each slice %\\Gamma_{a,b,c}% has weighted size 1 (and we have been referring to %\\mu% as “slices-equal measure” for this reason), and the whole cube %{}[3]^n% has weighted size equal to the %(n+1)^{th}% triangular number, %\\frac{(n+1)(n+2)}{2}%. Example: in %{}[3]^2%, the diagonal points 11, 22, 33 each have weighted size 1, whereas the other six off-diagonal points have weighted size 1/2\\. The total weighted size of %{}[3]^2% is 6. Let %c^\\mu_n% be the largest _weighted_ size of a line-free set. For instance, %c^\\mu_0 = 1%, %c^\\mu_1 = 2%, and %c^\\mu_2 = 4%. As in the unweighted case, every time we find a subset B of the grid %\\Delta_n := \\{ (a,b,c): a+b+c=n\\}% without equilateral triangles, it gives a line-free set %\\Gamma_B := \\bigcup_{(a,b,c) \\in B} \\Gamma_{a,b,c}%. The weighted size of this set is precisely the cardinality of B. Thus we have the lower bound %c^\\mu_n \\geq \\overline{c}^\\mu_n%, where %\\overline{c}^\\mu_n% is the largest size of equilateral triangles in %\\Delta_n%.\n\nSo, this **hyper-optimistic conjecture:** We in fact have %c^\\mu_n = \\overline{c}^\\mu_n%. In other words, to get the optimal weighted size for a line-free set, one should take a set which is a union of slices %\\Gamma_{a,b,c}%. If true, this will imply the DHJ theorem. Note also that all our best lower bounds for the unweighted problem to date have been unions of slices. Also, the k=2 analogue of the conjecture is true, and is known as the [LYM inequality](http://en.wikipedia.org/wiki/Lubell-Yamamoto-Meshalkin_inequality) (in fact, for k=2 we have %c^\\mu_n = \\overline{c}^\\mu_n = 1% for all n). If the conjecture is false, then perhaps this will be visible by computing upper and lower bounds for %c^\\mu_n, \\overline{c}^\\mu_n% for small n. By hand I can check that %\\overline{c}^\\mu_n = c^\\mu_n% for n=0,1,2… but perhaps with all the above progress we can also get good bounds for %n=3,4,5%?\n"},{"id":238,"owner_id":13,"created_at":"2016-02-20T05:00:00.000Z","contents":"Could you clarify the definition of %\\bar c_n^\\mu%? As far as generalizations of LYM go, let %d_{abc} = |A\\cap \\Gamma_{a,b,c}|%, and suppose %\\sum_{a+b+c=n} d_{abc}/|\\Gamma_{a,b,c}| \\le f(n)% for all line-free %A%. Michael’s examples seem to suggest %f(n)% grows quickly—as opposed to %f(n) = 1% in LYM – so, intuitively, how do we hope for something like LYM? I don’t think this is exactly what’s needed, but we do have %\\sum_{a+b+c=n} d_{abc}p_{abc} \\le 2% where %p_{abc} = (2^a+2^b+2^c-3)/(4^n-3^n)% is the percent of lines passing through any single element of %\\Gamma_{a,b,c}%."},{"id":239,"owner_id":1,"created_at":"2016-02-20T05:00:00.000Z","contents":"One can define %\\overline{c}^\\mu_n% in two equivalent ways. One of them is that it is the largest weighted size %\\mu(A)% of a line-free set %A \\subset [3]^n% which is the union of slices %\\Gamma_{a,b,c}%. (This is in contrast to %c^\\mu_n%, which is the largest weighted size of _any_ line-free set, not necessarily the union of slices.)\n\nAnother equivalent definition is that %\\overline{c}^\\mu_n% is the largest subset of the triangular grid %\\Delta_n := \\{ (a,b,c) \\in {\\Bbb Z}_+^3: a+b+c=n \\}% which contain no equilateral triangles. For instance, by deleting the two points %(0,0,2)% and %(1,1,0)% from %\\Delta_3% one removes all triangles, whereas removing one point is not enough, and so %\\overline{c}^\\mu_2 = 4%.\n\nAs for your second question, I don’t think we have yet a strategy for this, but one could imagine some sort of extremal combinatorics argument by showing that a line-free set which is not a union of slices can be “improved” to increase its weighted size, without losing the line-free property. Showing this rigorously, though, would be, well, hyper-optimistic. (Incidentally, what you call %f(n)% would be what I would call an upper bound for %c^\\mu_n%.)'\n"},{"id":240,"owner_id":5,"created_at":"2016-02-11T13:45:00.000Z","contents":"Am I understanding this right? Minimal deletion for %\\Delta_3% removes (0,3,0) (0,2,1) (2,1,0) (1,0,2), leaving 6 points. So %\\overline{c}^\\mu_3=6%?"},{"id":241,"owner_id":1,"created_at":"2016-02-11T22:02:00.000Z","contents":"Well, that certainly makes the set triangle-free, so %\\overline{c}^\\mu_3 \\geq 6%. If you know that you cannot make %\\Delta_3% triangle-free by removing only three points, then yes, %\\overline{c}^\\mu_3% would equal 6."},{"id":242,"owner_id":5,"created_at":"2016-02-11T22:12:00.000Z","contents":"Some proof: %\\Delta_3% cannot be triangle free removing only 3 points.\n\nYes, with only three removals each of these (non-overlapping) triangles must have one removal:  \n1. %(0,3,0) (0,2,1) (1,2,0)%\n2. %(0,1,2) (0,0,3) (1,0,2)%\n\n3. %(2,1,0) (2,0,1) (3,0,0)%\n\nConsider choices from 1:  \n* %(0,3,0)% leaves triangle %(0,2,1) (1,2,0) (1,1,1)%\n* %(0,2,1)% forces a second removal at %(2,1,0)% (otherwise there is triangle at %(1,2,0) (1,1,1) (2,1,0)%) but then none of the choices for third removal work\n* %(1,2,0)% is symmetrical with %(0,2,1)%"},{"id":244,"owner_id":5,"created_at":"2016-02-12T14:43:00.000Z","contents":"The %\\overline{c}^\\mu_3% case was originally proposed as a puzzle by Kobon Fujimura (who is best known for [Kobon Triangles](http://mathworld.wolfram.com/KobonTriangle.html)). I have been searching the recreational mathematics literature for any other mention of our problem but I haven’t found a reference."},{"id":245,"owner_id":5,"created_at":"2016-02-12T14:55:00.000Z","contents":"Closing this one out because I don't think there's any way forward with this approach."},{"id":246,"owner_id":1,"created_at":"2016-02-12T16:23:00.000Z","contents":"Hold up. I have an idea. If we want a \"ridicously-hyper-optimistic\" statement extending LYM to 3-letters alphabet we can take this:\n\nConsider all blocks %B% of slices %\\Gamma_{a,b,c}% not containing a combinatorial line. Let %S% be a subset of %\\{0,1,2\\}^n=\\Gamma% without a combinatorial line, then the sum of densities of %S% in some weighted collections of blocks which form a fractional covering of %\\Gamma% is at most\n"},{"id":247,"owner_id":5,"created_at":"2016-02-12T17:25:00.000Z","contents":"Good point. I'll open it back up then."}]},{"id":4,"title":"Szemerédi Approach","slug":"szemeredi-approach","created_at":"2016-02-20T05:00:00.000Z","created_by":2,"status":"open","timeline":[{"id":3211,"type":"comment","ref":452}],"comments":[{"id":452,"owner_id":3,"created_at":"2016-02-05T12:24:00.000Z","contents":"Wassup"}]},{"id":5,"title":"Obstructions to Uniformity","slug":"obstructions-to-uniformity","created_at":"2016-02-20T05:00:00.000Z","created_by":1,"status":"open","timeline":[{"id":4524,"type":"comment","ref":453},{"id":4525,"type":"comment","ref":454}],"comments":[{"id":453,"owner_id":0,"created_at":"2016-02-20T05:00:00.000Z","contents":"Yoyoyo"},{"id":454,"owner_id":1,"created_at":"2016-02-20T05:00:00.000Z","contents":"Yoyoyo"}]},{"id":6,"title":"Lower Bounds","slug":"lower-bounds","created_at":"2016-02-20T05:00:00.000Z","created_by":1,"status":"open","timeline":[{"id":4,"type":"comment","ref":1043},{"id":5,"type":"comment","ref":1044},{"id":6,"type":"comment","ref":1049},{"id":7,"type":"artifact","ref":456},{"id":8,"type":"comment","ref":1051},{"id":9,"type":"comment","ref":1052},{"id":10,"type":"action","ref":2},{"id":11,"type":"comment","ref":1053},{"id":12,"type":"action","ref":3},{"id":13,"type":"action","ref":4},{"id":14,"type":"comment","ref":1054},{"id":15,"type":"action","ref":5},{"id":16,"type":"action","ref":6},{"id":17,"type":"comment","ref":1055}],"comments":[{"id":1043,"owner_id":18,"created_at":"2016-02-16T05:43:00.000Z","contents":"There is a connection between %c_4%, %c_5% and %c_6%:\nOf the 450 length-six sequences defined in my last comment, 150 begin with a 1, and 52 begin with 12. The same thing applies with the current lower bounds of %c_7%, %c_8% and %c_9% It also gives lower bounds of 32864 for %c_1%, and 837850 for %c_1%\n"},{"id":1044,"owner_id":1,"created_at":"2016-02-16T07:36:00.000Z","contents":"This is quite interesting. I wonder if it is possible to extract an asymptotic for the lower bound obtained by this method as %n \\to \\infty%. It is also interesting to see that %c_n/3^n% is really decaying very slowly as %n \\to \\infty%. It may be time to set up some sort of communal spreadsheet for all this data. I'll have a look into this.\n\nIn light of the discussion in @gowers' threads, it seems that the [Kruskal-Katona theorem](http://en.wikipedia.org/wiki/Kruskal%E2%80%93Katona_theorem) is likely to be relevant here, at least for large %n%.\n\n@peake: Hmm. Does this mean that all of our optimal counterexamples are in fact nested (e.g. is the %c_6% example a slice of the %c_7% example?) This may be a transient phenomenon, but still an interesting one.\"\n"},{"id":1049,"owner_id":1,"created_at":"2016-02-16T08:09:00.000Z","contents":"Okay. I've set up a collaborative spreadsheet to track the current data we have for %c_n, c'_n, c''_n%. Should be editable to anyone whose already contributed. Let me know if you don't have access but want it. It should automatically propagate the bounds (1), (2) to any new bound entered. One should presumably be able to program the computations in 219 into here too. Please feel free to contribute to it.\",\n"},{"id":1051,"owner_id":15,"created_at":"2016-02-16T13:47:00.000Z","contents":"This has perhaps already been pointed out, but there is at most one entry in the [Encyclopedia of Integer Sequences](https://oeis.org/) consistent with what is already known about %c_n% (A052979). It matches %c_n% exactly where we already know the value, and matches the (current) bounds for at least the first fifteen entries."},{"id":1052,"owner_id":14,"created_at":"2016-02-16T18:19:00.000Z","contents":"Hey @ttao. I'm just joining now and hoping to help out. Mind giving me access to the spreadsheet? My email is on my profile."},{"id":1053,"owner_id":1,"created_at":"2016-02-16T19:05:00.000Z","contents":"@klas, done."},{"id":1054,"owner_id":1,"created_at":"2016-02-16T19:35:00.000Z","contents":"I’ve added the lower bound %c_6 \\geq 236% from this [paper of Edel](http://www.springerlink.com/content/m55136x765161240/). In [this other paper (by Bierbrauer)](http://www.mathi.uni-heidelberg.de/~yves/Papers/ABound.pdf), an inequality is obtained which in our notation reads (only for %n \\geq 3%):\n\n%%\\displaystyle c_n \\leq \\frac{3c_{n-1} + 1}{1 + c_{n-1}/3^{n-1}}%%\n\nWhich thus slightly improves on this problem. I’ll update the spreadsheet and the table accordingly.\n"},{"id":1055,"owner_id":18,"created_at":"2016-02-22T20:07:00.000Z","contents":"I've added few more lower bounds to Terry's spreadsheet. The sets of (abc) that I've used are the following for %n% a multiple of 3. I think they are triangle-free.\n\n1. For %n=3m-1%, restrict the first digit of a %3m% sequence to be 1;\n2. For %n=3m-2%, restrict the first two digits of a %3m% sequence to be 12;\n3. For %n < 21%, ignore any triple with a negative entry.\n"}]}],"actions":[{"id":2,"type":"share","created_at":"2016-02-20T05:00:00.000Z","actor_id":1,"actee_id":14,"artifact_id":456},{"id":3,"type":"reference","created_at":"2016-02-20T05:00:00.000Z","actor_id":1,"artifact_id":321},{"id":4,"type":"reference","created_at":"2016-02-20T05:00:00.000Z","actor_id":1,"artifact_id":325},{"id":5,"type":"add","created_at":"2016-02-20T05:00:00.000Z","actor_id":1,"artifact_id":456,"count":304},{"id":6,"type":"add","created_at":"2016-02-20T05:00:00.000Z","actor_id":18,"artifact_id":456,"count":153},{"id":7,"type":"resolve","created_at":"2016-02-20T05:00:00.000Z","actor_id":5},{"id":8,"type":"unresolve","created_at":"2016-02-20T05:00:00.000Z","actor_id":5}],"artifacts":[{"id":456,"reference":"linked","type":"dataset","owner_id":1,"title":"Precomputed Values for C","last_updated_at":"2016-02-20T05:00:00.000Z","source":{"url":"https://docs.google.com/spreadsheets/d/1q1_vEcLArwRk_31azk7w4FNbm1cqnbRgW3m7S9NM2tc/edit"}},{"id":321,"owner_id":1,"reference":"linked","type":"article","title":"Extensions of Generalized Product Caps","last_updated_at":"2016-02-09T07:41:00.000Z","metadata":{"abstract":"We give some variants of a new construction for caps. As an application of these constructions, we obtain a 1216-cap in PG(9,3) a 6464-cap in PG(11,3) and several caps in ternary affine spaces of larger dimension, which lead to better asymptotics than the caps constructed by Calderbank and Fishburn"},"author_id":17,"source":{"url":"http://www.springerlink.com/content/m55136x765161240/"}},{"id":325,"owner_id":1,"reference":"linked","type":"article","title":"Bounds on Affine Caps","last_updated_at":"2016-02-09T07:41:00.000Z","metadata":{"abstract":"A cap in affine space is a set of k-tuples so that whenever different elements align via equivalent conditions—any three similarly leaning tuples are linearly independent."},"author_id":16,"source":{"url":"http://www.mathi.uni-heidelberg.de/~yves/Papers/ABound.pdf"}},{"id":457,"reference":"linked","type":"document","owner_id":0,"title":"AMS Grant Proposal","last_updated_at":"2016-02-11T14:35:00.000Z","source":{"url":"https://docs.google.com/spreadsheets/d/1q1_vEcLArwRk_31azk7w4FNbm1cqnbRgW3m7S9NM2tc/edit"}}]},"one-way-functions":{"id":1,"title":"Do one-way functions exist?","slug":"one-way-functions","category":3,"config":{"tasks":true,"milestones":true},"summary":"Are there functions where the input value cannot be derived from the output value?\n","description":"One-way functions are fundamental tools for cryptography, personal identification, and other data security applications. While the existence of one-way functions in this sense is also an open conjecture, there are several candidates that have withstood decades of intense scrutiny.\n","significance":"Cryptography is built on the basis of one-way functions. Many modern applications are made of these principles—everything from securely storing passwords to ensuring you can access your bank's website without your browsing session being hijacked by someone else.\n","prior_work":"","citation":{"doi":"10.18258/3356"},"metrics":{"followers":273,"citations":46}},"p-v-np":{"id":2,"title":"P vs. NP","slug":"p-v-np","category":3,"config":{"tasks":true,"milestones":true},"summary":"If a problem's solution can be _verified_ quickly, can it also be _solved_ quickly?\n","description":"The P vs. NP problem asks whether every problem whose solution can be quickly _verified_ by a computer can also be quickly _solved_ by a computer. First mentioned in a letter between Kurt Gödel to John von Neumann, Gödel asked whether a certain NP-complete problem could be solved in quadratic or linear time. The precise statement of the P versus NP problem was introduced in 1971 by Stephen Cook in his seminal paper \"The complexity of theorem proving procedures.\"\n","significance":"Many of the problems we know to be in NP or NP-complete are problems that we actually want to solve, problems that arise, say, in circuit design or in other industrial design applications. Furthermore, since the diverse NP-complete problems are all polynomial time related to one another, if we should ever learn a feasible means of solving any of them, we would have feasible means for all of them. The result of this would be extraordinary, something like a second industrial revolution. It would be as though we suddenly had a huge permanent increase in computational power, allowing us to solve an enormous array of practical problems heretofore out of our computational reach. The P vs. NP question is important in part because of this tantalizing possibility.\n","prior_work":"","citation":{"doi":"10.18258/3356"},"metrics":{"followers":1498,"citations":18}},"shape-of-the-universe":{"id":3,"title":"What is the shape of the universe?","slug":"shape-of-the-universe","category":1,"config":{"tasks":true,"milestones":true},"summary":"How can we understand the way the universe has expanded?\n","description":"If the actual density of the universe is greater than the critical density, then it contains enough mass to eventually stop its expansion. In this case, the universe is closed and finite, though it has no end, and has a spherical shape. Once the universe stops expanding, it will begin to contract. Galaxies will stop receding and start moving closer and closer together. Eventually, the universe will undergo the opposite of the Big Bang, often called the \"Big Crunch.\" This is known as a closed universe.\n","significance":"","metrics":{"followers":45,"citations":0}},"supernova-origins":{"id":4,"title":"What are the processes that cause a supernova?","slug":"supernova-origins","category":1,"config":{"tasks":true,"milestones":true},"summary":"While astronomers know a lot about supernovas, how they originate remains unknown.\n","description":"The P vs. NP problem asks whether every problem whose solution can be quickly _verified_ by a computer can also be quickly _solved_ by a computer. First mentioned in a letter between Kurt Gödel to John von Neumann, Gödel asked whether a certain NP-complete problem could be solved in quadratic or linear time. The precise statement of the P versus NP problem was introduced in 1971 by Stephen Cook in his seminal paper \"The complexity of theorem proving procedures.\"\n","significance":"","citation":{"doi":"10.18258/3356"},"metrics":{"followers":8,"citations":1}}},"users":[{"id":0,"handle":"gowers","name":"Tim Gowers","title":"Professor","institution":"University of Cambridge","location":"Cambridge, UK","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/gowers.jpg"}},{"id":1,"handle":"ttao","name":"Terrence Tao","institution":"University of California, Los Angeles","location":"Los Angeles, CA","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/terrencetao.jpg"}},{"id":2,"handle":"jozsef","name":"Jozsef Solymosi","institution":"University of British Columbia","location":"Vancouver, Canada","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/jozsef.jpg"}},{"id":3,"handle":"gil","name":"Gil Kalai","institution":"Yale University","location":"New Haven, Connecticut","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/gil.jpg"}},{"id":4,"handle":"timaustin","name":"Tim Austin","institution":"NYU Courant Institute","location":"New York, USA","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/timaustin.jpg"}},{"id":5,"handle":"jasondyer","name":"Jason Dyer","location":"Phoenix, Arizona","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/jasondyer.jpg"}},{"id":6,"handle":"ryanodonnell","name":"Ryan O'Donnell","institution":"Carnegie Mellon","location":"Pittsburgh, Pennsylvania","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/rodonnell.jpg"}},{"id":7,"handle":"celsholtz","name":"Christian Elsholtz","location":"Darmstadt, Germany","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/celsholtz.jpg"}},{"id":8,"handle":"boris","name":"Boris Bukh","institution":"Carnegie Mellon","location":"Pittsburgh, Pennsylvania","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/boris.jpg"}},{"id":9,"handle":"wgasarch","name":"William Gasarch","institution":"University of Maryland","location":"New York, NY","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/wgasarch.jpg"}},{"id":10,"handle":"tneylon","name":"Tyler Neylon","institution":"New York University","location":"New York, NY","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/tneylon.jpg"}},{"id":11,"handle":"sune","name":"Sune Kristian Jakobsen","institution":"University of Oxford","location":"Oxford, UK","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/sune.jpg"}},{"id":12,"handle":"kevinobryant","name":"Kevin O'Bryant","institution":"CUNY","location":"New York, NY","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/kobryant.jpg"}},{"id":13,"handle":"tyler","name":"Tyler Neylon","institution":"NYU","location":"New York, NY","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/tneylon.jpg"}},{"id":14,"handle":"klas","name":"Klas Markstrom","institution":"Umeå University","location":"Umeå, Sweden","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/klas.jpg"}},{"id":15,"handle":"nielsen","name":"Michael Nielsen","location":"Toronto, Canada","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/nielsen.jpg"}},{"id":16,"handle":"jurgen","name":"Jürgen Bierbrauer","institution":"Michigan Technological University","location":"Houghton, Michigan","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/jurgen.jpg"}},{"id":17,"handle":"yves","name":"Yves Edel","institution":"Ghent University","location":"Gent, Belgium","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/yves.jpg"}},{"id":18,"handle":"peake","name":"Michael Peake","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/peake.jpg"}},{"id":19,"handle":"you","name":"Your Name","avatar":{"url":"http://polysoph-assets.s3.amazonaws.com/rzurowski.png"}}]}
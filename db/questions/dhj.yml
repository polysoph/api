---
id: 0
title: Finding a simple proof for the density Hales-Jewett theorem
slug: dhj-theorem
category: 0
status: closed
config:
  tasks: false
  milestones: true
keywords:
  - 0
  - 1
  - 2
  - 3
summary: Can we develop a proof for the [density Hales-Jewett](https://en.wikipedia.org/wiki/Hales%E2%80%93Jewett_theorem) theorem using only basic mathematical principles? How might this help us better understand the nature of prime numbers?
description: |
  Discovered in 1975, the density Hales-Jewett theorem (DHJ) is a technique for estimating line-free configurations in multi-dimensional grids. Furstenberg and Katznelson 1991 paper laid out the first mathematical proof for the relationship. However, while their proof is sound, it relies on advanced techniques in the branch of mathematics called [ergodic theory](https://en.wikipedia.org/wiki/Ergodic_theory), which makes it difficult to follow.

  ### Goal

  This research project aims to use [combinatorial mathematics](https://en.wikipedia.org/wiki/Combinatorics) to discover a more simple proof for the density Hales-Jewett theorem.

  ### Significance

  An important part of mathematics is finding proofs using simpler techniques. Simpler proofs let us learn more about the relationships behind theorems. And in the case of the DHJ, it's close ties with other theorems mean that having a simpler proof could lead us to new understandings of prime numbers, which are used in everything from cryptography to dealing with large sets of data.

  ### About the Theorem

  Although the proof of DHJ is complex, the basic statement can be understood by anyone. Take a look at the following three-by-three grid:

  ![A 9x9 grid with circles in some of the spaces. A line goes through the right 3 spaces of the grid.](http://polysoph-assets.s3.amazonaws.com/dhj-lines-01.png)

  By marking seven of the squares on the grid with a dot; as you can see, it’s possible to draw a line through three of those dots. By contrast, the configuration in the following picture is line-free—you can’t draw a line through any three of the dots:

  ![](http://polysoph-assets.s3.amazonaws.com/dhj-lines-02.png)

  After a while, you discover that this configuration is the largest possible line-free configuration. In particular, if you mark seven dots on the grid, then no matter how you place the dots, it is always possible to draw a line through three of the dots, somewhere on the grid.

  This logic extends into more dimensions. A three-dimensions 3x3 grid also contains a largest possible line-free configuration. And if we extend this into %n% dimensions, we eventually find that..."
significance: |
  While the density Hales-Jewett theorem has been proven several times in the past (see [Szemerédi, 1975](#!) and [Furstenberg and Katznelson, 1991](#!)), the proofs have traditionally been long and drawn-out. Helping to find a simple proof can lead to a better understanding of the relationship between the DHJ-Szemerédi and Szemerédi–Green-Tao theorems, which relate closely to how we understand prime numbers. Why should you care about DHJ? DHJ connects to the problem of understanding the structure of the prime numbers. Finding new proofs can give us significant new insights that help us understand why a result is true in the first place. Indeed, this is exactly what happened with the multiple proofs of Szemerédi’s theorem.
metrics:
  followers: 149
  citations: 13
citation:
  doi: "10.18258/3356"
milestones:
  - id: 0
    type: begin
    title: Project Began
    created_at: "2016-02-01T23:43:00.000Z"
    contents: ""
    owner_id: 0
    comments: 0
    views: 500
  - id: 1
    title: "Upper and Lower Bounds"
    created_at: "2016-02-09T15:12:00.000Z"
    contents: |
      Several of us have been working away on [the "Lower Bounds" discussion](/question/dhj-theorem/discussions/6), and have made some serious progress—but have now run into an equally serious issue. We started off exploring the relationship between the [Encyclopedia of Integer Sequences ](https://oeis.org/) values for 450 length sequences.

      For any positive integer %n%, %let {}[3]^n% be the set of strings of length %n% consisting of %1%s, %2%s, and %3%s, and define a combinatorial line to be a triple of such strings arising by taking a string in %\{1,2,3,x\}^n% with at least one wildcard %x%, and substituting %x=1%, %x=2%, %x=3% in that string (e.g. %xx1x3% would give the combinatorial line %\{11113, 22123, 33133\}%).  Call a set %A \subset [3]^n% of strings line-free if it contains no combinatorial lines, and let %c_n% be the size of the largest set %A% which is line-free. We then have arrive at:

      %%\lim_{n \to \infty} c_n/3^n = 0%%

      This theorem implies several other important results, most notably Roth’s theorem on length three progressions (in an arbitrary abelian group!) and also the corners theorem of Ajtai and Szemeredi (again in an arbitrary abelian group).

      Here are some of the questions we are initially exploring in the thread (which you can read through starting from comment [#3](http://localhost:8080/question/dhj-theorem/discussions/6#comment-1049) in the thread):

      1. What are the best upper and lower bounds for %c_n% for small %n%?  Currently we know %c_0=1%, %c_1=2%, %c_2=6%, %c_3=18%, %c_4=52%, with some partial results for higher %n%. We also have some relationships between different %c_n%.
      2. What do extremal or near-extremal sets (i.e. line-free sets with cardinality close to c_n) look like?
      3. What are some good constructions of line-free sets that lead to asymptotic lower bounds on c_n?  The best asymptotic lower bound we have currently is c_n \geq 3^{n - O( \sqrt{\log n} )}.
      4. Can these methods extend to other related problems, such as the Moser cube problem or the capset problem?
      5. How feasible is it to extend the existing combinatorial proofs of Roth’s theorem (or the corners theorem), in particular the arguments of Szemeredi, to the Density Hales-Jewett theorem?
      6. But I imagine that further threads might develop in the course of the discussion.

      As with the rest of the project, this is supposed to be an open collaboration: please feel free to pose a question or a comment, even if (or especially if) it is just barely non-trivial.
    owner_id: 1
    comments: 0
    views: 678
  - id: 2
    title: Quasirandomness
    owner_id: 0
    created_at: "2016-02-15T15:12:00.000Z"
    contents: |
      Many proofs of combinatorial density theorems rely in one way or another on an appropriate notion of quasirandomness. The idea is that, as we do, you have a dense subset of some structure and you want to prove that it must contain a substructure of a certain kind. You observe that a random subset of the given density will contain many substructures of that kind (with extremely high probability), and you then ask, “What properties do random sets have that cause them to contain all these substructures?” You look for some deterministic property of subsets of your structure that is sufficient to guarantee that a dense subset contains roughly the same number of substructures as a random subset of the same density.

      For example, suppose you are trying to find a triangle in a dense graph. (We can think of a graph with %n% vertices as a subset of the complete graph with %n% vertices. So the complete graph %K_n% is our “structure” in this case.) It turns out that a quasirandomness property that gives you the right number of triangles is this: a graph of density %\alpha% is quasirandom if the number of quadruples %(x_1,x_2,x_3,x_4)% such that %x_1x_2,x_2x_3,x_3x_4% and %x_4x_1% are all edges is approximately %\alpha^4n^4%. That is, the number of labelled 4-cycles is roughly what it would be in a random graph. If this is the case, then it can be shown that it is automatically also the case that the number of triples %(x_1,x_2,x_3)% such that %x_1x_2,x_2x_3% and %x_3x_1% are edges is approximately %\alpha^3n^3%, again the number that you would expect in a random graph.

      This might seem a slightly disappointing answer to the question. We are trying to guarantee the expected number of one small subgraph and we assume as our quasirandomness property something that appears to be equally strong: that we have the expected number of another small subgraph. How can that possibly be a good thing to do?

      A first answer is that if you have the right number of 4-cycles then it doesn’t just give you triangles: it gives you all small subgraphs. But an answer that is more pertinent to us here is that we can draw very useful consequences if a graph does not have the right number of 4-cycles. For example, we can find two large sets of vertices %A% and %B% such that the number of pairs %(x,y)\in A\times B% such that %xy% is an edge is significantly larger than %\alpha|A||B|% (which is what we would get for a random graph). Note that this is a global conclusion (an over-dense bipartite subgraph) deduced from a local hypothesis (the wrong number of 4-cycles). We can turn this round and say that we have an alternative definition of quasirandomness: a graph of density %\alpha% is quasirandom if for any two large sets %A% and %B% of vertices, the number of pairs %(x,y)\in A\times B% such that %xy% is an edge is about %\alpha |A||B|%. This definition turns out to be equivalent to the previous definition.

      This sort of local-to-global implication is extremely useful. The local property can be used to show that sets have substructures. And if the local property fails, then one has some global property of the set that can be exploited in a number of ways.

      It is not too important for the purposes of this thread to understand what all these ways are. But one of them does stand out: a global property is useful if it allows you to find a substructure of the main structure such that the intersection of %A% with that substructure is denser than it would be for a random set. In the graphs case, our substructure will be a large complete bipartite graph. The obvious substructure to hope for in the density Hales-Jewett set-up is a combinatorial subspace (which is like a combinatorial line except that you have several different variable sets, with the number of these sets being called the dimension of the combinatorial subspace).
    comments: 3
    views: 326
  - id: 5
    title: Possible Proof Strategies
    created_at: "2016-02-28T17:42:00.000Z"
    owner_id: 0
    contents: |
      I'll try to keep this summary brief, instead linking to comments that expand on what I say. If we take our lead from known proofs of Roth’s theorem and the corners theorem, then we can discern several possible approaches (each one attempting to imitate one of these known proofs).

      **Szemerédi’s Original Proof** ([Discussion](#!))

      Szemerédi proved a proof of a one-dimensional theorem. I do not know whether his proof has been modified to deal with the multidimensional case (this was not done by Szemerédi, but might perhaps be known to Terry, who recently digested and rewrote Szemerédi’s proof). So it is not clear whether one can base an entire argument for DHJ on this argument, but it could well contain ideas that would be useful for DHJ. For a sketch of Szemerédi’s proof, see [this post](http://example.com) of @ttao’s. He also has links to comments that expand on what he says.

      Ajtai and Szemerédi found a clever proof of the corners theorem that used Szemerédi’s theorem as a lemma. This gives us a direction to explore that we have hardly touched on. Very roughly, to prove the corners theorem you first use an averaging argument to find a dense diagonal. For any two such points %(x,r-x)% and %(y,r-y)% you know that the point %(x,r-y)% does not lie in %A% (assuming %A% is corner-free), which gives you some kind of non-quasirandomness.

      It allows you to find a large Cartesian product %X \times Y% in which %A% is a bit too dense. To exploit this, Ajtai and Szemerédi used Szemerédi’s theorem to find long arithmetic progressions %P \subset X% and %Q \subset Y% of the same common difference so that %A% has a density increment in %P \times Q%. So we could think about what the analogue would be in the DHJ setting. Presumably it would be some kind of multidimensional Sperner statement of sufficient depth to imply Szemerédi’s theorem. A naive suggestion would be that in a dense subset of %{}[2]^n% you can find a large-dimensional combinatorial subspace in which all the variable sets have the same size. If you apply this to a union of layers, then you find an arithmetic progression of layer-cardinalities. But this feels rather artificial, so here’s a question we could think about.

      **Density-increment Strategies** ([Discussion](#!))

      The idea here is to prove the result by means of the following two steps.

      1. If %A% does not contain a combinatorial line, then it correlates with a set %S% with some kind of atypical structure that we can describe.
      2. Every set %S% with that kind of structure can be partitioned into (or almost evenly covered by) a collection of combinatorial subspaces with dimension tending to infinity as %n% moves to infinity.

      Then, if %A% contains no combinatorial line, then find %S% such that %A% is a bit denser in %S% than it is in %{}[3]^n%. Cover %S% with subspaces. By averaging we find that the density of %A% is a bit too large in one of these subspaces. But now we are back where we started with a denser set %A% so we can repeat. The iteration must eventually terminate (since we cannot have a density greater than 1) so if the initial %n% was large enough then we must have had a combinatorial line.

      **Triangle Removal** ([Discussion](/question/dhj-theorem/discussions/0))

      This was the initial proposal, and we have not been concentrating on it recently, so I will simply defer to [the original post](/question/dhj-theorem/discussions/0), and add the remark that if we manage to obtain a complete and global description of [obstructions to uniformity](/question/dhj-theorem/discussions/4), then regularity and triangle removal could be an alternative way of using this information to prove the theorem. And in the case of corners, this is a somewhat simpler thing to do.

      **Ergodic-inspired methods** ([Discussion](#!))

      For a proposal to base a proof on at least some of the ideas that come out of the proof of Furstenberg and Katznelson, see @ttao's comment 439, as well as the first few comments after it. @ttao has also begun an [online reading seminar](http://terrytao.wordpress.com/2009/02/11/a-reading-seminar-on-density-hales-jewett/) on the Furstenberg-Katznelson proof, using not just the original paper of Furstenberg and Katznelson but also a more recent paper of Randall McCutcheon that explains how to finish off the Furstenberg-Katznelson argument.

      P.S. We've heard that this platform is going to be adding a wiki feature soon. We'll keep you posted, and we'll be able to start documenting things there.
    comments: 5
    views: 435
owners:
  - user_id: 0
  - user_id: 1
discussions:
  - id: 0
    title: Initial Work
    slug: initial-work
    status: resolved
    created_at:
    created_by: 0
    timeline:
      - id: 1
        type: comment
        ref: 401
      - id: 2
        type: comment
        ref: 402
      - id: 3
        type: comment
        ref: 403
      - id: 4
        type: comment
        ref: 404
      - id: 5
        type: comment
        ref: 405
      - id: 6
        type: comment
        ref: 406
      - id: 7
        type: comment
        ref: 407
      - id: 8
        type: action
        ref: 9
    comments:
      - id: 401
        owner_id: 0
        created_at: "2016-02-01T13:43:00.000Z"
        contents: |
          To get the ball rolling, I'll be giving my initial thoughts in small chunks.

          Might it be possible to prove the density Hales-Jewett theorem for %k=3% by imitating the triangle-removal proof that dense subsets of %[n]^2% contain corners? If one were going to do that, what would the natural analogue of the tripartite graph associated with a dense subset be? If we’ve got a dense subset %A% of %[3]^n%, then how might we define a tripartite graph %G% in such a way that triangles in %G% correspond to (possibly degenerate) combinatorial lines?

          It’s perhaps worth pointing out that there is a natural definition of a degenerate combinatorial line: it’s one where the set of coordinates that vary from 1 to 3 is empty. If we just look at the vertex sets %X% and %Y% in the case of subsets %A\subset [n]^2%, we have a very simple definition for an edge: it’s a pair %(x,y)% that belongs to %A%.
      - id: 402
        owner_id: 0
        created_at: "2016-02-01T13:45:00.000Z"
        contents: Does %[3]^n% split up nicely as a Cartesian product? You can write it as %[3]^r\times [3]^s%, but that doesn’t help much, because if we represent a typical point as %(x,y)% with %x\in [3]^r% and %y\in [3]^s%, then a Hales-Jewett line is not a triple of the form %(x,y)%, %(x+d,y)%, %(x,y+d)%. That’s partly because we can’t even make sense of adding %d%, but more fundamentally because even if we could invent a useful definition of addition in this context, we still wouldn’t get a Hales-Jewett line.
      - id: 403
        owner_id: 0
        created_at: "2016-02-01T13:49:00.000Z"
        contents: |
          By the way, there’s a nice way of deducing the corners result from density Hales-Jewett: Suppose we take a set %A\subset [3]^n% with the special property that whether or not a sequence %x% belongs to %A% depends only on the numbers of %1%s, %2%s and %3%s in %x%. That is, let %T% be the set of all triples %(r,s,t)% of non-negative integers such that %r+s+t=n%, let %S% be some subset of %T%, and let %A% be the set of sequences %x% such that %(n_1(x),n_2(x),n_3(x))\in S%, where %n_i(x)% stands for the number of is in %x%. Now suppose you have a combinatorial line %L%. If %d% is the number of variable coordinates in %L%, then %S% must contain a triple of points of the form %(r+d,s,t)%, %(r,s+d,t)%, %(r,s,t+d)% (where %r%, %s% and %t% are the numbers of %1%s, %2%s and %3%s amongst the fixed coordinates of %L%).

          Now %T% is a large triangular subset of a triangular grid, and one can (if one really feels like it) shear it so that it becomes a right-angled triangle instead of an isosceles triangle. This way it's not hard to show that the assertion that every dense subset of %T% contains the vertices of an equilateral triangle of the form %(r+d,s,t)%, %(r,s+d,t)%, %(r,s,t+d)% is equivalent to the corners result.

          Unfortunately it’s not the case that dense subsets of %T% correspond to dense subsets of %[3]^n% (because almost every point in %[3]^n% has similar numbers of %1%s, %2%s and %3%s) but there are dodges for getting round that.
      - id: 404
        owner_id: 1
        created_at: "2016-02-01T13:49:00.000Z"
        contents: |
          @gil made a blog post on this project, the %k=2% case of density Hales-Jewett is actually just [Sperner’s theorem](http://en.wikipedia.org/wiki/Sperner_family).

          He made the point that, of course, this theorem already has a combinatorial proof, but this proof has a rather different flavour than usual triangle removal lemma (or its %k=2% counterpart, which is the trivial statement that if a subset of a set V of vertices is small, then it can be removed entirely by removing a small number of vertices.)

          As pointed out to me by my student, Le Thai Hoang, another difference between DHJ and, say, the corners problem, is that one does not expect a positive density of all combinatorial lines to lie in the set. For instance, in the %k=2% case, and identifying %{}[2]^n% with the set of subsets of %{1,2,…,n}%, Hoang observed that the set of subsets of %{1,…,n}% of cardinality %n/2 + O( \sqrt{n} )% has positive density, but only about %(2+o(1))^n% of the %3^n% combinatorial lines here actually lie in the set. This seems to fit well with your observations that some weighting of the vertices etc. is needed.

          At the ergodic theory level, there is also a distinction between the ergodic theory proofs of, say, the corners theorem, and of density Hales-Jewett, which may be an indication that something different happens at the combinatorial level too. Namely, in the ergodic proof of corners (or Roth, or Szemeredi, etc.) one has a single dynamical system (with the action of a single group G), and one works with that system (and its factors) throughout the proof. But for DHJ, one has a system of measurable sets indexed by words, and one repeatedly refines the set of words to much smaller subsets in order to obtain good “stationarity” properties. (My student, @timaustin, could explain this much better than I could.)

          It is only once one has all this stationarity that the proof can really begin in earnest. This is also consistent with proofs of the colouring Hales-Jewett theorem, which also massively restrict the space of lines being considered, and with the above observation that we do not expect a positive density of the full space of lines to be contained in the set. So perhaps some preliminary “regularisation” of the problem is needed before doing anything triangle-removal-like.
      - id: 405
        owner_id: 0
        created_at: "2016-02-01T13:49:00.000Z"
        contents: |
          @gil, if you're joining this on this project, I just want to make a quick remark about Fourier expansions and the %k=3% case. I got stuck several years ago when I was trying to develop a Fourier approach to a similar problem. Maybe with your deep knowledge of this kind of thing you can get me unstuck again. The problem was that the natural Fourier basis in

          %\emptyset [3]^n% was the basis you get by thinking of %\emptyset [3]^n% as the group %\mathbb{Z}_3^n%. And if that’s what you do, then there appear to be examples that do not behave quasirandomly, but which do not have large Fourier coefficients either. For example, suppose that %n% is a multiple of 7, and you look at the set %A% of all sequences where the numbers of 1s, 2s and 3s are all multiples of 7\. If two such sequences lie in a combinatorial line, then the set of variable coordinates for that line must have cardinality that’s a multiple of 7, from which it follows that the third point automatically lies in the line. So this set %A% has too many combinatorial lines.

          But I’m fairly sure — perhaps you can confirm this — that %A% has no large Fourier coefficient. You can use this idea to produce lots more examples. Obviously you can replace 7 by some other small number. But you can also pick some arbitrary subset %W% of %\emptyset[n]% and just ask that the numbers of 0s, 1s and 2s inside %W% are multiples of 7. If these observations are correct, then they suggest another interesting subproblem:

          > Describe all obstructions to randomness. That is, find a nice explicit set %\mathcal{F}% of functions defined on %\emptyset [3]^n% such that (the characteristic function of) any set with the wrong number of combinatorial lines correlates with a function in %\mathcal{F}%.

          Ideally, correlation with a function in %\mathcal{F}% would imply a density increase on some subset. Of course, this would be aiming for a Roth-type proof rather than a Ruzsa-Szemerédi-Solymosi-type proof.
      - id: 406
        owner_id: 3
        created_at: "2016-02-01T15:49:00.000Z"
        contents: |
          Hi all. I'm currently focused on my own project, but I can definitely lurk around and provide some thoughts on a few of the approaches.
      - id: 407
        owner_id: 1
        created_at: "2016-02-01T15:49:00.000Z"
        contents: |
          Glad you're joining us @gil.

          I think it might be time to split this discussion up into several different ones. We've identified at least 3 approaches here (Triangle Removal in #401, Szémeredi in #404, and Fourier expansions in #405).

          I've created discussions for each—lets continue these conversations there.

  - id: 1
    title: Triangle Removal
    slug: triangle-removal
    status: open
    created_at: "2016-02-02T02:08:00.000Z"
    created_by: 2
    timeline:
      - id: 234
        type: comment
        ref: 2
      - id: 454
        type: comment
        ref: 9
      - id: 463
        type: comment
        ref: 74
      - id: 335
        type: comment
        ref: 75
      - id: 452
        type: comment
        ref: 85
      - id: 353
        type: comment
        ref: 95
      - id: 356
        type: comment
        ref: 96
    comments:
      - id: 2
        owner_id: 2
        created_at: "2016-02-02T02:08:00.000Z"
        contents: "We should consider a variant of the original problem first. If the removal technique doesn’t work here, then it won’t work in the more difficult setting. If it works, then we have a nice result! Consider the Cartesian product of an %IP_d% set.\n\n> An %IP_d% set is generated by %d% numbers by taking all the %2^d% possible sums. So, if the %n% numbers are independent then the size of the %IP_d% set is %2^d%. In the following statements we will suppose that our %IP_d% sets have size %2^n%.\n\nProve that for any %c>0% there is a %d%, such that any %c%-dense subset of the Cartesian product of an %IP_d% set (it is a two dimensional pointset) has a corner.\n\nThe statement is true. One can even prove that the dense subset of a Cartesian product contains a square, by using the density HJ for %k=4%\\. (I will sketch the simple proof later) What is promising here is that one can build a not-very-large tripartite graph where we can try to prove a removal lemma. The vertex sets are the vertical, horizontal, and slope -1 lines, having intersection with the Cartesian product. Two vertices are connected by an edge if the corresponding lines meet in a point of our c-dense subset. Every point defines a triangle, and if you can find another, non-degenerate, triangle then we are done. This graph is still sparse, but maybe it is well-structured for a removal lemma.\n\nFinally, let me prove that there is square if %d% is large enough compare to %c%. Every point of the Cartesian product has two coordinates, a %0,1% sequence of length %d%. It has a one to one mapping to %[4]^d%; Given a point %((x_1,\\ldots,x_d),(y_1,\\ldots,y_d))% where %x_i,y_j% are 0 or 1, it maps to %(z_1,…,z_d)%, where %z_i=0% if %x_i=y_i=0%, %z_i=1% if %x_i=1% and %y_i=0%, %z_i=2% if %x_i=0% and %y_i=1%, and finally %z_i=3% if %x_i=y_i=1%. Any combinatorial line in %[4]^d% defines a square in the Cartesian product, so the density HJ implies the statement."
      - id: 9
        owner_id: 0
        created_at: "2016-02-02T06:36:00.000Z"
        contents: "If you follow the triangle-removal proof of the corners result, you find that you can prove something stronger: in a set of density %\\delta% you don’t just get one corner, but you get %c(\\delta)n^2% corners. (The proof is that if there are very few corners then there are very few triangles in the associated tripartite graph, and then you can apply triangle removal just as before to get a contradiction.) If one is going to find an analogous proof here, it is essential that it should not try to prove a lemma that has false consequences. In our case, we must be careful that we don’t accidentally try to develop an approach that would also imply that a positive density of all possible combinatorial lines lie in the set.\n\nI haven’t checked, but I think that weighting the vertices as suggested in KK avoids this pitfall, because now the analogous set to the one used by Le Thai Hoang is concentrated around sets of size %n/3%. Probably this is exactly what Terry meant at the end of his second paragraph."
      - id: 74
        owner_id: 0
        created_at: "2016-02-03T20:26:00.000Z"
        contents: "Actually, I was trying to prove a Varnavides-type result not because we need it but because it was a sort of reality check: if we can’t get such a result then it is unlikely that triangle removal will work. But since I’m now fairly confident that we can get such a result, I want to see what happens if one just goes ahead and tries to apply some kind of triangle-removal lemma in the case where %A% is a dense subset of a large triangular grid of slices. So let’s fix notation again. I’m going to suppose that %A% is a dense subset of the union of all slices %\\gamma_{a+r,b+s,c+t}%, where %(a,b,c)% is some fixed element of %T_{n-m}% and %(r,s,t)% ranges over %T_m%. Also, I’m assuming that %a,b,c% are all around %n/3% so that all slices have about the same size. Now I want to apply the trick from comments N and P. So what is the graph now? Instead of taking %X% to be the full power set of %[n]% I want to take just those sets that could conceivably occur as the set of 1s in a sequence in %A%. And that’s all sets of size %a+r%, where %0\\leq r\\leq m%. Similarly, %Y% consists of all sets of size between %b% and %b+m%, and %Z% consists of all sets of size between %c% and %c+m%. The edges are as before: I join %U\\in X% to %V\\in Y% if the sequence that’s 1 on %U% and 2 on %V% and 3 everywhere else belongs to %A%. And similarly for the other two bipartite graphs. And clearly a non-degenerate triangle in this graph gives rise to a combinatorial line. I’ll think about the implications of this in my next comment."
      - id: 75
        owner_id: 0
        created_at: "2016-02-03T20:44:00.000Z"
        contents: "I didn’t mention it explicitly, but of course a necessary condition for joining %U\\in X% to %V\\in Y% is still that %U% and %V% should be disjoint. So the first reason that we are not yet done is that the graphs are still very sparse, since two random sets of size %n/3% have a tiny probability of being disjoint. So the question is, has anything been gained by restricting attention to a large triangular grid of slices? Something’s been gained from the Varnavides point of view, I think, but has anything been gained from the triangle-removal point of view?\n\nAt the moment it feels, rather disappointingly, as though _nothing_ has been gained. It’s still the case that the graph where you join two sets if they are disjoint is importantly non-quasirandom, since the remark made in comment LL carries over to this context too. I would hazard a guess that there are two morals to draw from this.\n\n1. It may still be quite convenient to restrict to a triangular grid of slices, in order to make certain distributions uniform.\n2. We are probably forced to consider obstructions to uniformity. In fact, there’s a different way of justifying.\n3. At some point, if a triangle-removal argument is going to work, we will have to prove a statement to the effect that if you have three bipartite graphs that are all “regular” and “dense”, then they must give rise to lots of triangles.\n\nIn the normal dense-graphs case, “regular” just means quasirandom, and it even suffices for just one of the three bipartite graphs to be regular, provided there are plenty of vertices in the third set with high degree in both of the first two sets. In our case, there is a one-to-one correspondence between edges of one of the bipartite graphs and points in %[3]^n%. So this suggests that at some point we are going to have to understand what a “regular” or “quasirandom” subset of %[3]^n% is like. To be continued in the next comment."
      - id: 85
        owner_id: 1
        created_at: "2016-02-03T23:46:00.000Z"
        contents: "The lower bound %c_4 \\geq 49% you have compares pretty well with the upper bound %c_4 \\leq 3c_3 = 54%. It may be possible to shave the upper bound down a notch, by trying to figure out what the extremal subsets of %{}[3]^3% are that have 18 elements but no combinatorial lines. Any 54-element subset of %{}[3]^4% with no combinatorial lines must be extremal on every three-dimensional slice, which looks unlikely if there are few extremals."
      - id: 95
        owner_id: 0
        created_at: "2016-02-04T04:18:00.000Z"
        contents: "That's an important question, but I’d like to understand more precisely what you mean by it.\n\nTo begin with, let me distinguish carefully between two interpretations of the question. One would be “Is it possible to do without any analogue of the triangle removal lemma from this point on?” and another would be “Is it unnatural to try to use the triangle removal lemma from this point on?”\n\nThe background to the question is that if a positive proportion of the slices is rich, then by the corners theorem one can find a triple of rich slices of the form %\\gamma_{a+r,b,c}%, %\\gamma_{a,b+r,c}%, %\\gamma_{a,b,c+r}%. So one might take the view that you use the triangle removal lemma (via the corners theorem) to get yourself to a triple of rich slices and then let other methods take over from there. Now we haven’t actually said what we mean by “rich”. It has to mean more than “intersects densely with %A%” since it is easy to find examples where %A% intersects three such slices densely but there is no combinatorial line. (See e.g. comment 20.) And it seems unlikely that there is a definition of “rich” such that:\n\n1. Every dense set %A% intersects a dense set of slices richly and\n2. Given any triangle of slices intersected richly by %A% you must get a combinatorial line from that triangle of slices.\n\n If you don’t get a dense set of rich slices, then perhaps you can pass to substructures (e.g. by fixing a few coordinates) where something like density or energy improves and try again. This would be a sort of mixed argument: first use density/energy-increase arguments to get to a situation where lots of slices are rich (which I see as some kind of quasirandomness property, but perhaps I shouldn’t), then apply the corners theorem to get a triple of rich slices, and finally apply some rich-slices counting lemma to get a combinatorial line. (I’m not at all sure that that’s what Terry had in mind, and would be interested to know.)\n\nAnyhow, perhaps that suggests that triangle removal could be used just to get a triple of good slices, and other arguments could be used to do the rest. In my next comment I’ll discuss the other interpretation of Terry’s question."
      - id: 96
        owner_id: 0
        created_at: "2016-02-04T04:53:00.000Z"
        contents: "Is it clearly wrong to try to prove the whole theorem via a triangle-removal lemma? I think that the argument gains considerably in force because it is possible to define a tripartite graph in such a way that nondegenerate triangles correspond to combinatorial lines in %A%. Moreover, the construction naturally generalizes the tripartite graph that you construct in order to deduce the corners theorem from the triangle removal lemma.\n\nHere’s a way one might justify this. For a long time it was known that the triangle removal lemma implied Roth’s theorem, but until Jozsef came along, nobody had observed that it would also do corners. Before that one might have reasoned as follows. Let us label the lines of slope 1 in %[n]^2% according to the value of %x-y%. If we have a corner, then the labels of the lines that it lies in must form an arithmetic progression of length 3.\n\nAnd indeed, since our dense subset of %[n]^2% might even _be_ a union of lines of slope 1, we see that the corners theorem implies Roth’s theorem. So perhaps we should use Roth’s theorem to find a rich triple of diagonals and then use other arguments to obtain a corner. Is this a fair analogy or does it break down somewhere? Before giving up on triangle removal, I would want answers to the following questions:\n\n1. Is it possible to formulate a triangle-removal statement that isn’t obviously false and that implies that a tripartite graph contains lots of triangles (because it contains lots of edge-disjoint degenerate ones)?\n2. If so, is there the slightest chance of proving the statement by imitating the proof of the usual triangle-removal lemma? Or does the sparseness and nonquasirandomness of the bipartite graph where we join two sets if they are disjoint make a project of this kind hopeless? I could perhaps add a third question, of a slightly different kind.\n3. Is it the case that in order to prove that a regular triple contained many triangles, one would have to develop tools (connected with obstructions to uniformity) that could be used to give an easier proof that didn’t go via triangle removal, regularity lemmas etc.?\n\nI feel reasonably optimistic about (1) if one restricts to a triangular grid of slices. I’m much less sure about (2) and (3)."
  - id: 2
    title: "Varnavides Approach"
    slug: varnavides-approach
    created_at: "2016-02-02T09:08:00.000Z"
    created_by: 1
    status: resolved
    timeline:
      - id: 256
        type: comment
        ref: 13
      - id: 257
        type: comment
        ref: 14
      - id: 258
        type: comment
        ref: 16
      - id: 259
        type: comment
        ref: 17
      - id: 260
        type: comment
        ref: 18
      - id: 261
        type: comment
        ref: 24
      - id: 262
        type: comment
        ref: 25
      - id: 263
        type: comment
        ref: 63
      - id: 264
        type: comment
        ref: 64
      - id: 265
        type: action
        ref: 11
    comments:
      - id: 13
        owner_id: 1
        created_at: "2016-02-02T09:08:00.000Z"
        contents: "Perhaps in order to understand the issue coming from Hoang’s observation a bit better, one can pose the following subproblem: what is the natural analogue of Varnavides theorem for DHJ? Varnavides used Roth’s theorem as a black box to show that any subset of %{}[n]% of density %\\delta% contained %\\geq c(\\delta) n^2% 3-term arithmetic progressions when n was large; a similar argument using the Ajtai-Szemeredi theorem as a black box also shows that any subset of %{}[n]^2% of density %\\delta% contains %\\geq c(\\delta) n^3% corners.\n\nSo, using DHJ as a black box, what is the natural implication as to how “numerous” combinatorial lines are in a dense set? It seems that direct cardinality of such lines may not be the best measure of being “numerous”, but there should presumably still be some Varnavides-like theorem out there."
      - id: 14
        owner_id: 2
        created_at: "2016-02-02T10:26:00.000Z"
        contents: "Regarding @ttao's last comment about a Varnavides-like theorem, I’m a bit skeptical. Even for the %k=2% case (Sperner’s thm) if we take points of the d dimensional cube where the difference between the 0-s and 1-s is at most %\\sqrt{d}% then we have a positive fraction of the elements and there are less than %\\binom{d/2}{\\sqrt{d}}2^d% combinatorial lines. Probably this should be the right magnitude for %k=2% as a Varvadines-like result and it shouldn’t be difficult to prove. (But I don’t have the proof …)"
      - id: 16
        owner_id: 1
        created_at: "2016-02-02T13:00:00.000Z"
        contents: "Hmm, you’re right; so we get a bound of %c_n \\gg 3^n / \\sqrt{n}% asymptotically.\n\n@jozsef, yes, we will have to normalise things appropriately to get a Varnavides type theorem. For the %k=3% problem, for instance, one might try to shoot for a lower bound for the number of combinatorial lines which is something like %(3+o(1))^n% rather than %4^n%. Alternatively, we could somehow weight each line, perhaps by some factor depending on how many wildcards it contains.\n\n(Note that any set of density %\\delta% in a large %{}[3]^n% will contain at least one combinatorial line in which the number of wildcards is at most %C(\\delta)%, since one can partition %{}[3]^n% into copies of %{}[3]^{n_0}%, where %n_0% is the first index for which DHJ holds at density %\\delta/2%, and observe that the original set will have density at least %\\delta/2% in at least %\\delta/2% of these copies. So maybe one has to weight things to heavily favour lines with very few wildcards.)"
      - id: 17
        owner_id: 1
        created_at: "2016-02-02T13:46:00.000Z"
        contents: "It just occurred to me that if a subset of %{}[3]^n% has positive density, then by the central limit theorem we may restrict attention to those strings which have %n/3+O(\\sqrt{n})% 1’s, 2’s, and 3’s, and still have a positive density of strings remaining. So without loss of generality we may assume that all strings in the set are “balanced” in this manner. Hence, the only combinatorial lines which really matter are those which have %O(\\sqrt{n})% wildcards in them, rather than %O(n)%. So the number of combinatorial lines that are genuinely “in play” is just %(3+o(1))^n% rather than %4^n%. I wonder if DHJ implies the existence of a combinatorial line with %\\Theta(\\sqrt{n})% wildcards. In the k=2 case it seems that a double counting argument should be able to establish something like this, though I haven’t checked it thoroughly. If so, then there seems to be a good shot at getting the “right” Varnavides type theorem."
      - id: 18
        owner_id: 1
        created_at: "2016-02-02T14:01:00.000Z"
        contents: "Two more thoughts… firstly, a sufficiently good Varnavides type theorem for DHJ may have a separate application from the one in this project, namely to obtain a “relative” DHJ for dense subsets of a sufficiently pseudorandom subset of %{}[3]^n%, much as I did with Ben Green for the primes (and which now has a significantly simpler proof by @gowers and by Reingold-Trevisan-Tulsiani-Vadhan). There are other obstacles though to that task (e.g. understanding the analogue of “dual functions” for Hales-Jewett), and so this is probably a bit off-topic.\n\nAnother thought: For any %a,b,c% adding up to %n%, let %\\gamma_{a,b,c}% be the subset of %{}[3]^n% consisting of those strings with a 1s, b 2s, and c 3s. If A is a dense subset of %{}[3]^n%, then %A \\cap \\gamma_{a,b,c}% should be a dense subset of %\\gamma_{a,b,c}% for many a, b, c; call a triple (a,b,c) _rich_ if this is the case. By applying the corners theorem (!) we should then be able to find %(a+r,b,c)%, %(a,b+r,c)%, %(a,b,c+r)% which are simultaneously rich. But now this might be a good place with which to try a triangle removal argument, converting the lines of r wildcards connecting %\\gamma_{a+r,b,c}, \\gamma_{a,b+r,c}, \\gamma_{a,b,c+r}% into triangles as Tim suggests in the main post?"
      - id: 24
        owner_id: 0
        created_at: "2016-02-02T17:45:00.000Z"
        contents: "I want to continue the discussion in #13, #14, #16, #17 and #18 concerning the appropriate Varnavides-type theorem for density Hales-Jewett. The basic difficulty appears to be that if you know that a point lies in a combinatorial line, then it affects the shape of the point.\n\nLet’s see this first in a very simple context, where we just put the uniform distribution on the set of all points and the uniform distribution on the set of all lines. We can represent a combinatorial line as a point in the space %\\{1,2,3,*\\}^n%, where the asterisks represent the varying coordinates. In this way we see easily that there are %4^n% combinatorial lines and that the set of fixed coordinates that take a given one of the values 1, 2 or 3 will typically have size about %n/4%. (For anyone who isn’t familiar with this kind of argument, if you choose a random combinatorial line, regarded as a sequence that consists of 1s, 2s, 3s and asterisks, then the set of places where you choose 1 is a random subset of %[n]% where each point is chosen with probability %1/4%. With very high probability such a set has size close to %n/4%.) So we see that while a typical point has roughly equal numbers of 1s, 2s and 3s, a typical combinatorial line contains points that have around %n/4% of two values and around %n/2% of the third value.\n\nSuppose we try to remedy this by putting a different distribution on to the set of lines. Is there some natural way to do this that encourages the set of variable coordinates to be much smaller than the sets of fixed coordinates? A rather unpleasant approach would be simply to put some upper bound on the size of the set of variable coordinates and otherwise choose uniformly. The most natural thing I can think of to do is not all that much nicer, but at least it smooths the cutoff. It’s to choose the variable set of coordinates by picking each point independently with probability %p%, where %p% is something like %n^{-1/2}%, and then choose the fixed coordinates randomly to be 1, 2 or 3\\. Actually, I can already see that this doesn’t work. The size of the resulting set of variable coordinates will be strongly concentrated around %n^{1/2}%. But it’s not hard to choose a dense set of sequences such that if you choose any two of them, %x% and %y%, then the number of 1s in %x% and the number of 1s in %y% do not differ by approximately %n^{1/2}%. The number of combinatorial lines in such a set would not be dense. The fact that the natural measure on the set of sequences does not project properly to the natural measure on the set of triples %(r,s,t)% of integers such that %r+s+t=n% seems to be a fundamental difficulty. Another thought one might have is to put a different measure on the set of points, so as to encourage points where we get two values roughly %n/4% times and one value roughly %n/2% times. But the trouble with this is that it leads to the no-degenerate-triangles problem. To be precise, it would lead to three sets of sequences (according to whether 1, 2 or 3 was the preferred value) and no sequence would belong to all three sets. So it really does seem essential to force the variable set to be small somehow. And all I have to suggest so far is to do something very unnatural like first randomly choosing the cardinality according to some distribution that isn’t too concentrated about any value — the most natural one I can think of is a gentle exponential decay — then choosing a set of that cardinality, and then choosing the fixed coordinates randomly. But I have no reason to suppose that that leads to a Varnavides-type theorem."
      - id: 25
        owner_id: 0
        created_at: "2016-02-02T18:44:00.000Z"
        contents: "I see now that there’s quite a substantial overlap between my last comment and #16\\. But one can view my last comment as an elaboration of his, perhaps.\n\nHere I just want to make the quick suggestion that one way to work out what the appropriate Varnavides theorem should say is to prove it first. How would this strategy work? Well, we assume the density Hales-Jewett theorem in the following form: for every %\\delta>0% there exists %k% such that every subset of %[3]^k% of size at least %(\\delta/2)3^k% contains a combinatorial line. (The %\\delta/2% is convenient in just a moment.)\n\nNow let %A% be a subset of %[3]^n% of size at least %\\delta 3^n%. Here, %n% is an integer that is much bigger than %k%. The set %[3]^n% can be covered uniformly by structures that are isomorphic to %[3]^k%, and a positive proportion (in fact, %\\delta/2% at least) of these structures intersect %A% in a subset of density at least %\\delta/2%. So a positive proportion of these structures contain a combinatorial line. And now a double-counting argument gives us lots of combinatorial lines.\n\nNow there are several choices to make if one wants to pursue this line of attack. The main one is what set of %k%-dimensional substructures of %[3]^n% to average over. For the benefit of people who haven’t thought too hard about Hales-Jewett, let me give a pretty general class of such structures. You choose %k% disjoint sets %V_1,V_2,\\ldots,V_k%, and you choose fixed values in %\\{1,2,3\\}% for all elements of %[n]% that do not belong to one of the %V_i%. The %k%-dimensional structure consists of the %3^k% sequences that take the fixed values outside the %V_i% and are constant on each %V_i%. What can we deduce from the fact that there exists %k% that depends on %\\delta% only such that if %A% intersects any %k%-dimensional set of this kind in at least %(\\delta/2)3^k% places then that %k%-dimensional set contains a combinatorial line in %A%?\n\nThe choice one has is this: over what set of structures of this kind (or with what weighting on the structures) do we do an averaging argument? And what can we expect to get out of it at the end? Is it likely to give anything useful?\n\nOne other question: is there a reasonably precise argument to the effect that if the triangle-removal approach worked, then it would imply a stronger Varnavides-type result? And if so, what would that result be? This could be another way to get a handle on the problem."
      - id: 63
        owner_id: 1
        created_at: "2016-02-03T08:04:00.000Z"
        contents: "I thought it might also be worth reporting here on some things I tried that _don’t_ seem to work, though perhaps I am missing something. Firstly, I tried improving the lower bound we now have on %c_n% by not taking the global statistics a, b, c of 1s, 2s, and 3s (and their resulting sets %\\gamma_{a,b,c}%, but instead partitioning [n] into subsets, using the local statistics for each subsets to create local versions of the %\\gamma%‘s, and then somehow gluing it all back together again. This failed to improve upon what we already have, and I think the reason is while the global %\\gamma_{a,b,c}% have no combinatorial lines, the same is not true of their local analogues (unless one takes a Cartesian product of several local %\\gamma%‘s, but this seems to be rather lossy.) Indeed, I still don’t know of any good example of a really large set with no combinatorial lines that isn’t somehow built out of the %\\gamma%‘s. The other thing I toyed with was trying to use the machinery of shifting and compression (in the spirit of Frankl, or Bollobas-Leader, or one of my papers with Ben Green) to try to get good answers to the k=2 problems (e.g. finding a good “robust” version of Sperner’s theorem, e.g. a Varnavides version). But this seems not to work too well. The problem seems to be that the extremisers for Sperner’s theorem don’t look much like downsets (they seem to be more like the boundary of a downset, though I don’t see how to exploit this). In any event, these techniques are quite specific to k=2 and would be unlikely to shed much light on k=3 except perhaps by analogy."
      - id: 64
        owner_id: 6
        created_at: "2016-02-03T08:28:00.000Z"
        contents: |
          On the topic of more analytic proofs of Sperner: I think I now see why Boris mentioned Kruskal-Katona in the same breath as Sperner.

          Let %n% be odd for simplicity and write %B_j% for the set of strings in %\\{0,1\\}^n% with exactly %j% %1%‘s. Let %k = \\lfloor n/2 \\rfloor%. Let %A_j% denote %A \\cap B_j% and let %\\mu_j% denote %A_j%‘s density within %B_j%. One particular thing Sperner tells us is that if %\\mu_k + \\mu_{k+1} > 1% then %A% contains a “combinatorial line” (because the overall density of %A% is at least that of %B_{k}%‘s density within %\\{0,1\\}^n%). Here is a way to see that with Kruskal-Katona: K-K implies that the upper-shadow %\\partial^+ A_k% of %A_k% has density at least %\\mu_k% within %B_{k+1}%.

          Now together %\\partial^+ A_k% and %A_{k+1}% have density exceeding %1% within %B_{k+1}%, hence they overlap. — Indeed, I think you can get most of Sperner out of extending this argument. Suppose that %A%‘s overall density exceeds that of %B_{k+1}% within %\\{0,1\\}^n%. As an example, perhaps %A% has %40\\%% of the strings in %B_{k}%, %40\\%% of the strings in %B_{k+5}% and %40\\%% of the strings in %B_{k+7}%. Now K-K implies that %A_{k}%‘s upper shadow is also at least %40\\%% of %B_{k+1}%. And the upper shaddow of that is at least %40\\%% (basically) of %B_{k+2}%. Etc. When you get up to %B_{k+5}%, you either get a collision and are done, or you have at least %80\\%% of %B_{k+5}%. Then you get at least %80\\%% of %B_{k+6}%, and then you have %120\\%% of %B_{k+7}% and you get a collision and are done.

          The reason I kind of like this proof is that there is an “analytic” proof of Kruskal-Katona that seems amenable to “robustification”. Although to be honest, I can’t quite remember how we got talking about this subject just now…
  - id: 3
    title: A Hyper Optimistic Conjecture
    slug: a-hyper-optimistic-conjecture
    created_at: '0001-02-20T05:00:00.000Z'
    created_by: 1
    status: open
    timeline:
      - id: 3411
        type: comment
        ref: 237
      - id: 3413
        type: comment
        ref: 238
      - id: 3414
        type: comment
        ref: 239
      - id: 3415
        type: comment
        ref: 240
      - id: 3416
        type: comment
        ref: 241
      - id: 3417
        type: comment
        ref: 242
      - id: 3418
        type: comment
        ref: 244
      - id: 3418
        type: comment
        ref: 245
      - id: 3419
        type: action
        ref: 7
      - id: 3419
        type: comment
        ref: 246
      - id: 3421
        type: comment
        ref: 247
      - id: 3420
        type: action
        ref: 8
    comments:
      - id: 237
        owner_id: 1
        created_at: '2016-02-20T05:00:00.000Z'
        contents: |
          Over in the [other thread](/question/dhj-theorem/discussions/0#comment-9), @gil and @gowers have proposed a “hyper-optimistic” conjecture, and asked whether it could be falsified using the examples from this thread. Let me rephrase so it's easier to follow.

          Given a set %A \subset {}[3]^n%, define the _weighted size_ %\mu(A)% of A by the formula:
          %%\mu(A) := \sum_{a+b+c=n} |A \cap \Gamma_{a,b,c}|/|\Gamma_{a,b,c}|%%
          Thus each slice %\Gamma_{a,b,c}% has weighted size 1 (and we have been referring to %\mu% as “slices-equal measure” for this reason), and the whole cube %{}[3]^n% has weighted size equal to the %(n+1)^{th}% triangular number, %\frac{(n+1)(n+2)}{2}%. Example: in %{}[3]^2%, the diagonal points 11, 22, 33 each have weighted size 1, whereas the other six off-diagonal points have weighted size 1/2\. The total weighted size of %{}[3]^2% is 6. Let %c^\mu_n% be the largest _weighted_ size of a line-free set. For instance, %c^\mu_0 = 1%, %c^\mu_1 = 2%, and %c^\mu_2 = 4%. As in the unweighted case, every time we find a subset B of the grid %\Delta_n := \{ (a,b,c): a+b+c=n\}% without equilateral triangles, it gives a line-free set %\Gamma_B := \bigcup_{(a,b,c) \in B} \Gamma_{a,b,c}%. The weighted size of this set is precisely the cardinality of B. Thus we have the lower bound %c^\mu_n \geq \overline{c}^\mu_n%, where %\overline{c}^\mu_n% is the largest size of equilateral triangles in %\Delta_n%.

          So, this **hyper-optimistic conjecture:** We in fact have %c^\mu_n = \overline{c}^\mu_n%. In other words, to get the optimal weighted size for a line-free set, one should take a set which is a union of slices %\Gamma_{a,b,c}%. If true, this will imply the DHJ theorem. Note also that all our best lower bounds for the unweighted problem to date have been unions of slices. Also, the k=2 analogue of the conjecture is true, and is known as the [LYM inequality](http://en.wikipedia.org/wiki/Lubell-Yamamoto-Meshalkin_inequality) (in fact, for k=2 we have %c^\mu_n = \overline{c}^\mu_n = 1% for all n). If the conjecture is false, then perhaps this will be visible by computing upper and lower bounds for %c^\mu_n, \overline{c}^\mu_n% for small n. By hand I can check that %\overline{c}^\mu_n = c^\mu_n% for n=0,1,2… but perhaps with all the above progress we can also get good bounds for %n=3,4,5%?
      - id: 238
        owner_id: 13
        created_at: '2016-02-20T05:00:00.000Z'
        contents: Could you clarify the definition of %\bar c_n^\mu%? As far as generalizations of LYM go, let %d_{abc} = |A\cap \Gamma_{a,b,c}|%, and suppose %\sum_{a+b+c=n} d_{abc}/|\Gamma_{a,b,c}| \le f(n)% for all line-free %A%. Michael’s examples seem to suggest %f(n)% grows quickly—as opposed to %f(n) = 1% in LYM – so, intuitively, how do we hope for something like LYM? I don’t think this is exactly what’s needed, but we do have %\sum_{a+b+c=n} d_{abc}p_{abc} \le 2% where %p_{abc} = (2^a+2^b+2^c-3)/(4^n-3^n)% is the percent of lines passing through any single element of %\Gamma_{a,b,c}%.
      - id: 239
        owner_id: 1
        created_at: '2016-02-20T05:00:00.000Z'
        contents: |
          One can define %\overline{c}^\mu_n% in two equivalent ways. One of them is that it is the largest weighted size %\mu(A)% of a line-free set %A \subset [3]^n% which is the union of slices %\Gamma_{a,b,c}%. (This is in contrast to %c^\mu_n%, which is the largest weighted size of _any_ line-free set, not necessarily the union of slices.)

          Another equivalent definition is that %\overline{c}^\mu_n% is the largest subset of the triangular grid %\Delta_n := \{ (a,b,c) \in {\Bbb Z}_+^3: a+b+c=n \}% which contain no equilateral triangles. For instance, by deleting the two points %(0,0,2)% and %(1,1,0)% from %\Delta_3% one removes all triangles, whereas removing one point is not enough, and so %\overline{c}^\mu_2 = 4%.

          As for your second question, I don’t think we have yet a strategy for this, but one could imagine some sort of extremal combinatorics argument by showing that a line-free set which is not a union of slices can be “improved” to increase its weighted size, without losing the line-free property. Showing this rigorously, though, would be, well, hyper-optimistic. (Incidentally, what you call %f(n)% would be what I would call an upper bound for %c^\mu_n%.)'
      - id: 240
        owner_id: 5
        created_at: '2016-02-11T13:45:00.000Z'
        contents: Am I understanding this right? Minimal deletion for %\Delta_3% removes (0,3,0) (0,2,1) (2,1,0) (1,0,2), leaving 6 points. So %\overline{c}^\mu_3=6%?
      - id: 241
        owner_id: 1
        created_at: '2016-02-11T22:02:00.000Z'
        contents: Well, that certainly makes the set triangle-free, so %\overline{c}^\mu_3 \geq 6%. If you know that you cannot make %\Delta_3% triangle-free by removing only three points, then yes, %\overline{c}^\mu_3% would equal 6.
      - id: 242
        owner_id: 5
        created_at: '2016-02-11T22:12:00.000Z'
        contents: "Some proof: %\\Delta_3% cannot be triangle free removing only 3 points.\n\nYes, with only three removals each of these (non-overlapping) triangles must have one removal:  \n1. %(0,3,0) (0,2,1) (1,2,0)%\n2. %(0,1,2) (0,0,3) (1,0,2)%\n\n3. %(2,1,0) (2,0,1) (3,0,0)%\n\nConsider choices from 1:  \n* %(0,3,0)% leaves triangle %(0,2,1) (1,2,0) (1,1,1)%\n* %(0,2,1)% forces a second removal at %(2,1,0)% (otherwise there is triangle at %(1,2,0) (1,1,1) (2,1,0)%) but then none of the choices for third removal work\n* %(1,2,0)% is symmetrical with %(0,2,1)%"
      - id: 244
        owner_id: 5
        created_at: '2016-02-12T14:43:00.000Z'
        contents: The %\overline{c}^\mu_3% case was originally proposed as a puzzle by Kobon Fujimura (who is best known for [Kobon Triangles](http://mathworld.wolfram.com/KobonTriangle.html)). I have been searching the recreational mathematics literature for any other mention of our problem but I haven’t found a reference.
      - id: 245
        owner_id: 5
        created_at: '2016-02-12T14:55:00.000Z'
        contents: Closing this one out because I don't think there's any way forward with this approach.
      - id: 246
        owner_id: 1
        created_at: '2016-02-12T16:23:00.000Z'
        contents: |
          Hold up. I have an idea. If we want a "ridicously-hyper-optimistic" statement extending LYM to 3-letters alphabet we can take this:

          Consider all blocks %B% of slices %\Gamma_{a,b,c}% not containing a combinatorial line. Let %S% be a subset of %\{0,1,2\}^n=\Gamma% without a combinatorial line, then the sum of densities of %S% in some weighted collections of blocks which form a fractional covering of %\Gamma% is at most
      - id: 247
        owner_id: 5
        created_at: '2016-02-12T17:25:00.000Z'
        contents: Good point. I'll open it back up then.

  - id: 4
    title: Szemerédi Approach
    slug: szemeredi-approach
    created_at: '2016-02-20T05:00:00.000Z'
    created_by: 2
    status: open
    timeline:
      - id: 3211
        type: comment
        ref: 452
      - id: 3212
        type: action
        ref: 10
    comments:
      - id: 452
        owner_id: 1
        created_at: '2016-01-05T12:24:00.000Z'
        contents: |
          Continuing from [the earlier discussion](/question/dhj-theorem/discussions/0#comment-404) about Szemeredi’s approach, it's worth noting that Endre also has a simple combinatorial proof of Roth’s theorem (which avoids Fourier analysis or the regularity lemma), instead relying on the Szemeredi cube lemma. This can for instance be found in Graham-Rothschild-Spencer’s book, or Chapter 10.7 of my book with Van, [or in Ernie’s writeup](http://www.math.gatech.edu/~ecroot/szemeredi.pdf).

          The basic idea is to observe that if a set A in %\emptyset [N]% is dense, then it must contain a high-dimensional cube Q. If A has no arithmetic progressions, then %Q – A% must be essentially disjoint from A. Expressing the d-dimensional cube %Q% as the final element of a sequence %Q_0 \subset Q_1 \subset \ldots \subset Q_d%, where each %Q_i% is one dimension larger than the preceding %Q_{i-1}%, and using the pigeonhole principle, we can find i such that %2 \cdot Q_i - A% has roughly the same size as %2 \cdot Q_{i+1} - A%. We can write %Q_{i+1} = Q_i + \{0,v_i\}% for some step %v_i%, and we thus see that %2 \cdot Q_i - A% is very stable under shifting by %v_i%, and thus both it and its complement can be partitioned into long arithmetic progressions of step %v_i%. Since A is disjoint from %2 \cdot Q_i - A%, it is squeezed into the other progressions, and on one of these we have a density increment.

          Now apply the usual density increment argument and we are done. It’s not clear whether this argument can be adapted to give the corners theorem, let alone the %k=3% case of DHJ, but perhaps it should be looked at.
  - id: 5
    title: Obstructions to Uniformity
    slug: obstructions-to-uniformity
    created_at: '2016-02-20T05:00:00.000Z'
    created_by: 1
    status: open
    timeline:
      - id: 4524
        type: comment
        ref: 453
      - id: 4525
        type: comment
        ref: 454
      - id: 4526
        type: comment
        ref: 455
      - id: 4527
        type: comment
        ref: 456
    comments:
      - id: 453
        owner_id: 0
        created_at: '2016-02-20T05:00:00.000Z'
        contents: |
          What does it mean to “understand what a quasirandom subset of %\emptyset [3]^n% is like?

          Here’s an attempt to explain. We are looking for a _definition_ of quasirandomness. My experience with other problems suggests that this definition should have the following property:

          > If %A% is a dense quasirandom subset of %\emptyset [3]^n% and %B% is an arbitrary dense subset of %\emptyset [3]^n%, then there is a combinatorial line %(x,y,z)% with %x\in A%, %y\in B% and %z\in B%.

          Indeed, one should have not just _one_ such combinatorial line but a _dense set_ of such combinatorial lines. (Here is a place where having a [Varnavides-type statement](/question/dhj-theorem/discussions/2) is actually a big help and not just a mere reassurance.)

          Experience with other problems also suggests that if %A% does _not_ have this property then there will be some simple reason for it. An example of such a reason is that %x_1% might be 1 for every %x\in A%. (This would fail to have the property in (ii) because we could take %B% to be the set of all sequences with %x_1=2%.) If one can think of all possible “simple reasons,” then it is often possible to pass to substructures or partitions of the ground set and obtain density or energy increases.

          (If you don’t know what this means, it’s not too important at this stage. The main point is that if you can do then there are standard techniques for using what you’ve done. In many other contexts, once you’ve got this the problem is basically solved.)
      - id: 454
        owner_id: 1
        created_at: '2016-02-20T05:00:00.000Z'
        contents: |
          Similar definitions apply if we restrict to a triple of slices or a triangular grid of slices.

          **Obstruction 1**. In any combinatorial line, a coordinate must either take all three values or just one. Therefore, a set %A% is not uniform unless the number of sequences %x% in %A% such that %x_i=j% is roughly %|A|/3% for every %i\in[n]% and %j\in\{1,2,3\}%. To prove this seems to be not quite trivial. An obvious set %B% to consider is to take %j% such that the number of points with %x_i=j% is significantly less than %|A|/3% and let %B% be the set of all sequences %x% such that %x_i=j%. Then the only points in %A% that can make lines with %B% are ones with %x_i=j%. Therefore, if we get the right number of combinatorial lines in %A\times B^2%, then we can drop to the subset %A''% that consists of those points only and we find that we get more combinatorial lines than we would expect (given the densities of the restrictions of %A% and %B% to %x_i=j%). I think one can mess around with this information and obtain a set %B% that proves that %A% is not uniform.

          Given that that was slightly tricky, I’ll restrict attention for now to what I might call _strong_ obstructions to uniformity. I’ll call a set %A% _strongly non-uniform_ if I can find a dense set %B% such that there are _no_ combinatorial lines %(x,y,z)% with %x\in A%, %y\in B% and %z\in B%. So finding more strong obstructions really is a genuinely elementary problem (in the sense of not needing special knowledge).
      - id: 455
        owner_id: 0
        created_at: '2016-02-21T07:00:00.000Z'
        contents: |
          **Strong obstruction 1**. If there exist %i% and %j% such that %x_i=j% for every %x% in %A%, then %A% is strongly non-uniform.
          **Proof**: let %k\ne j% and let %B% be the set of all sequences such that %x_i=k%. You can’t have a combinatorial line in which the %i%th coordinate is %j% once and %k% twice.

          **Strong obstruction 2**. If there is a positive integer %m% such that for every %x\in A% the number of 1s, 2s and 3s in %A% is always the same mod %m%, then %A% is strongly non-uniform.
          **Proof**: If %y% and %z% belong to a combinatorial line and have the same number of 1s, 2s and 3s mod %m%, then the size of the variable set must be a multiple of %m%. Therefore, we can choose %B% to be a set where the sequences again have fixed numbers of 1s, 2s and 3s mod %m%, but different numbers from those of %A%.

          **Strong obstruction 3**. This generalizes the previous example. Suppose that for every %x\in A% we know that the number of 1s belongs to a set %E_1% of residues mod %m%, the number of 2s belongs to a set %E_2%, and the number of 3s belongs to a set %E_3%. Now let %F_1,F_2,F_3% be sets of residues mod %m%, and let %B% be the set of all sequences with the number of 1s in %F_1% etc.

          **Strong obstruction 4**. One can generalize further by passing to a subset. That is, instead of looking at the numbers of 1s, 2s and 3s in the whole sequence, one just looks at the numbers of 1s, 2s and 3s in some specified set %X% of coordinates. The proof is more or less the same. I haven’t tried hugely hard to find more obstructions.

          Does anyone know of any? Does @boris's comment from before have any implications here?
      - id: 456
        owner_id: 8
        created_at: '2016-02-21T07:43:00.000Z'
        contents: |
          @ttao, for your definition of triangular grid in #454, to make sure I understood it properly, should it say %a+b+c\leq n-m% instead of %a,b,c\leq n-m%?

          @gowers, for the obstructions to the uniformity mentioned in #455, doesn’t it follow from that [discussion about Varnavides](/question/dhj-theorem/discussions/2) that to talk about pseudorandom sets we want our sets to be dense subsets in a suitable union of %\Gamma_{a,b,c}% that live near the middle of the lattice?
  - id: 6
    title: Lower Bounds
    slug: lower-bounds
    created_at: '2016-02-20T05:00:00.000Z'
    created_by: 1
    status: open
    timeline:
      - id: 4
        type: comment
        ref: 1043
      - id: 5
        type: comment
        ref: 1044
      - id: 6
        type: comment
        ref: 1049
      - id: 7
        type: artifact
        ref: 456
      - id: 8
        type: comment
        ref: 1051
      - id: 9
        type: comment
        ref: 1052
      - id: 10
        type: action
        ref: 2
      - id: 11
        type: comment
        ref: 1053
      - id: 12
        type: action
        ref: 3
      - id: 13
        type: action
        ref: 4
      - id: 14
        type: comment
        ref: 1054
      - id: 15
        type: action
        ref: 5
      - id: 16
        type: action
        ref: 6
      - id: 17
        type: comment
        ref: 1055
    comments:
      - id: 1043
        owner_id: 18
        created_at: "2016-02-16T05:43:00.000Z"
        contents: |
          There is a connection between %c_4%, %c_5% and %c_6%:
          Of the 450 length-six sequences defined in my last comment, 150 begin with a 1, and 52 begin with 12. The same thing applies with the current lower bounds of %c_7%, %c_8% and %c_9% It also gives lower bounds of 32864 for %c_1%, and 837850 for %c_1%
      - id: 1044
        owner_id: 1
        created_at: "2016-02-16T07:36:00.000Z"
        contents: |
          This is quite interesting. I wonder if it is possible to extract an asymptotic for the lower bound obtained by this method as %n \to \infty%. It is also interesting to see that %c_n/3^n% is really decaying very slowly as %n \to \infty%. It may be time to set up some sort of communal spreadsheet for all this data. I'll have a look into this.

          In light of the discussion in @gowers' threads, it seems that the [Kruskal-Katona theorem](http://en.wikipedia.org/wiki/Kruskal%E2%80%93Katona_theorem) is likely to be relevant here, at least for large %n%.

          @peake: Hmm. Does this mean that all of our optimal counterexamples are in fact nested (e.g. is the %c_6% example a slice of the %c_7% example?) This may be a transient phenomenon, but still an interesting one."
      - id: 1049
        owner_id: 1
        created_at: "2016-02-16T08:09:00.000Z"
        contents: |
          Okay. I've set up a collaborative spreadsheet to track the current data we have for %c_n%. Should be editable to anyone whose already contributed. Let me know if you don't have access but want it. It should automatically propagate to any new bound entered. One should presumably be able to program the computations in 219 into here too. Please feel free to contribute to it.",
      - id: 1051
        owner_id: 15
        created_at: "2016-02-16T13:47:00.000Z"
        contents: "This has perhaps already been pointed out, but there is at most one entry in the [Encyclopedia of Integer Sequences](https://oeis.org/) consistent with what is already known about %c_n% (A052979). It matches %c_n% exactly where we already know the value, and matches the (current) bounds for at least the first fifteen entries."
      - id: 1052
        owner_id: 14
        created_at: "2016-02-16T18:19:00.000Z"
        contents: "Hey @ttao. I'm just joining now and hoping to help out. Mind giving me access to the spreadsheet? My email is on my profile."
      - id: 1053
        owner_id: 1
        created_at: "2016-02-16T19:05:00.000Z"
        contents: "@klas, done."
      - id: 1054
        owner_id: 1
        created_at: "2016-02-16T19:35:00.000Z"
        contents: |
          I’ve added the lower bound %c_6 \geq 236% from this [paper of Edel](http://www.springerlink.com/content/m55136x765161240/). In [this other paper (by Bierbrauer)](http://www.mathi.uni-heidelberg.de/~yves/Papers/ABound.pdf), an inequality is obtained which in our notation reads (only for %n \geq 3%):

          %%\displaystyle c_n \leq \frac{3c_{n-1} + 1}{1 + c_{n-1}/3^{n-1}}%%

          Which thus slightly improves on this problem. I’ll update the spreadsheet and the table accordingly.
      - id: 1055
        owner_id: 18
        created_at: "2016-02-22T20:07:00.000Z"
        contents: |
          I've added few more lower bounds to Terry's spreadsheet. The sets of (abc) that I've used are the following for %n% a multiple of 3. I think they are triangle-free.

          1. For %n=3m-1%, restrict the first digit of a %3m% sequence to be 1;
          2. For %n=3m-2%, restrict the first two digits of a %3m% sequence to be 12;
          3. For %n < 21%, ignore any triple with a negative entry.
actions:
  - id: 2
    type: share
    created_at: "2016-02-20T05:00:00.000Z"
    actor_id: 1
    actee_id: 14
    artifact_id: 456
  - id: 3
    type: reference
    created_at: "2016-02-20T05:00:00.000Z"
    actor_id: 1
    artifact_id: 321
  - id: 4
    type: reference
    created_at: "2016-02-20T05:00:00.000Z"
    actor_id: 1
    artifact_id: 325
  - id: 5
    type: add
    created_at: "2016-02-20T05:00:00.000Z"
    actor_id: 1
    artifact_id: 456
    count: 304
  - id: 6
    type: add
    created_at: "2016-02-20T05:00:00.000Z"
    actor_id: 18
    artifact_id: 456
    count: 153
  - id: 7
    type: resolve
    created_at: "2016-02-20T05:00:00.000Z"
    actor_id: 5
  - id: 8
    type: unresolve
    created_at: "2016-02-20T05:00:00.000Z"
    actor_id: 5
  - id: 9
    type: resolve
    created_at: "2016-02-17T05:00:00.000Z"
    actor_id: 1
  - id: 10
    type: reference
    created_at: "2016-02-20T05:00:00.000Z"
    actor_id: 1
    artifact_id: 458
  - id: 11
    type: resolve
    created_at: "2016-02-17T05:00:00.000Z"
    actor_id: 6

artifacts:
  - id: 456
    reference: linked
    type: dataset
    owner_id: 1
    title: Precomputed Values for C
    last_updated_at: "2016-02-20T05:00:00.000Z"
    source:
      url: "https://docs.google.com/spreadsheets/d/1q1_vEcLArwRk_31azk7w4FNbm1cqnbRgW3m7S9NM2tc/edit"
  - id: 321
    owner_id: 1
    reference: linked
    type: article
    title: Extensions of Generalized Product Caps
    last_updated_at: "2016-02-09T07:41:00.000Z"
    metadata:
      abstract: We give some variants of a new construction for caps. As an application of these constructions, we obtain a 1216-cap in PG(9,3) a 6464-cap in PG(11,3) and several caps in ternary affine spaces of larger dimension, which lead to better asymptotics than the caps constructed by Calderbank and Fishburn
    author_id: 17
    source:
      url: http://www.springerlink.com/content/m55136x765161240/
  - id: 325
    owner_id: 1
    reference: linked
    type: article
    title: "Bounds on Affine Caps"
    last_updated_at: "2016-02-09T07:41:00.000Z"
    metadata:
      abstract: A cap in affine space is a set of k-tuples so that whenever different elements align via equivalent conditions—any three similarly leaning tuples are linearly independent.
    author_id: 16
    source:
      url: "http://www.mathi.uni-heidelberg.de/~yves/Papers/ABound.pdf"
  - id: 457
    reference: linked
    type: document
    owner_id: 0
    title: AMS Grant Proposal
    last_updated_at: "2016-02-11T14:35:00.000Z"
    source:
      url: "https://docs.google.com/spreadsheets/d/1q1_vEcLArwRk_31azk7w4FNbm1cqnbRgW3m7S9NM2tc/edit"
  - id: 458
    reference: linked
    type: article
    title: "Szemeredi’s Theorem on Three Term Progressions, at a Glance"
    owner_id: 0
    last_updated_at: "2016-02-11T14:35:00.000Z"
    source:
      url: "http://www.math.gatech.edu/~ecroot/szemeredi.pdf"
